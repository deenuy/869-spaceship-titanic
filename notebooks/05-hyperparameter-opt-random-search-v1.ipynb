{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z8miWz8zKnb7",
    "outputId": "0907a8f6-e265-4c13-ae73-450819bfb8cc"
   },
   "source": [
    "%cd /content\n",
    "!git clone https://github.com/deenuy/869-spaceship-titanic.git\n",
    "# !git pull origin main"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -r /content/869-spaceship-titanic/requirements.txt"
   ],
   "metadata": {
    "id": "nJgwiMENc30Y"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preliminaries: Inspect and Set up environment\n",
    "\n",
    "No action is required on your part in this section. These cells print out helpful information about the environment, just in case."
   ],
   "metadata": {
    "id": "RJOzHNdg8XHo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# üß∞ General-purpose libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "\n",
    "# üß™ Scikit-learn preprocessing & pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# üîç Scikit-learn model selection\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold\n",
    ")\n",
    "\n",
    "# üß† Scikit-learn classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# üöÄ Gradient boosting frameworks\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# üìä Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# üß™ Sample dataset (for testing/demo)\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n"
   ],
   "metadata": {
    "id": "-eeFtKdG8WRK",
    "ExecuteTime": {
     "end_time": "2025-06-06T19:40:24.455195Z",
     "start_time": "2025-06-06T19:40:24.022470Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# LOAD TRAINING DATA WITH SELECTED FEATURES\n",
    "# ================================================================\n",
    "\n",
    "print(\"üì• LOADING TRAINING DATA WITH SELECTED FEATURES\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Load complete processed training dataset\n",
    "df_processed = pd.read_csv('../data/processed/train_features_engineered.csv')\n",
    "print(f\"   ‚úÖ Training dataset loaded: {df_processed.shape}\")\n",
    "\n",
    "# Load selected features from feature selection phase\n",
    "best_features = pd.read_csv('../data/processed/best_features_selected.csv')['best_features'].tolist()\n",
    "print(f\"   ‚úÖ Selected features loaded: {len(best_features)} features\")\n",
    "\n",
    "# Filter training data using selected features only\n",
    "X_train = df_processed[best_features]\n",
    "y_train = df_processed['Transported']\n",
    "\n",
    "print(f\"\\nüìä FILTERED TRAINING DATA SUMMARY:\")\n",
    "print(f\"   üìä Original features: {df_processed.shape[1] - 2}\")  # Exclude PassengerId and Transported\n",
    "print(f\"   üìä Selected features: {X_train.shape[1]}\")\n",
    "print(f\"   üìä Feature reduction: {((df_processed.shape[1] - 2 - X_train.shape[1]) / (df_processed.shape[1] - 2) * 100):.1f}%\")\n",
    "print(f\"   üìä Training samples: {X_train.shape[0]}\")\n",
    "print(f\"   üìä Target distribution: {y_train.sum()}/{len(y_train)} transported\")\n",
    "\n",
    "print(f\"\\nüéØ SELECTED FEATURES FOR OPTIMIZATION:\")\n",
    "for i, feature in enumerate(best_features, 1):\n",
    "    print(f\"   {i:2d}. {feature}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training data ready for hyperparameter optimization!\")"
   ],
   "metadata": {
    "id": "UGwYX_gccUTq",
    "ExecuteTime": {
     "end_time": "2025-06-06T19:40:24.514316Z",
     "start_time": "2025-06-06T19:40:24.495496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• LOADING TRAINING DATA WITH SELECTED FEATURES\n",
      "=======================================================\n",
      "   ‚úÖ Training dataset loaded: (8693, 19)\n",
      "   ‚úÖ Selected features loaded: 6 features\n",
      "\n",
      "üìä FILTERED TRAINING DATA SUMMARY:\n",
      "   üìä Original features: 17\n",
      "   üìä Selected features: 6\n",
      "   üìä Feature reduction: 64.7%\n",
      "   üìä Training samples: 8693\n",
      "   üìä Target distribution: 4378/8693 transported\n",
      "\n",
      "üéØ SELECTED FEATURES FOR OPTIMIZATION:\n",
      "    1. HomePlanet\n",
      "    2. CryoSleep\n",
      "    3. RoomService\n",
      "    4. TotalSpend\n",
      "    5. LuxurySpend\n",
      "    6. Cabin_HomePlanet\n",
      "\n",
      "‚úÖ Training data ready for hyperparameter optimization!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### STEP 1: Hyperparameter Tuning of XGBoost for Accuracy Optimization using Random Search\n",
    "This step fine-tunes the XGBoost model using RandomizedSearchCV with 5-fold cross-validation, targeting improved accuracy aligned with leaderboard evaluation. The objective is to efficiently identify high-performing configurations for deployment while maintaining generalizability and avoiding overfitting through stochastic hyperparameter exploration.\n",
    "Hyperparameter Tuning of XGBoost using Random Search, includes feature engineering integrated into a pipeline using ColumnTransformer. This version includes:\n",
    "\n",
    "* Imputation and scaling for numeric features\n",
    "* Imputation and one-hot encoding for categorical features\n",
    "* Modular pipeline with XGBoost\n",
    "* Random search over relevant hyperparameter distributions\n",
    "* Accuracy as the scoring metric\n",
    "* Efficient parameter space exploration through uniform random sampling\n",
    "* Configurable number of iterations for computational budget control\n",
    "* Unbiased coverage of hyperparameter combinations without exhaustive enumeration\n"
   ],
   "metadata": {
    "id": "uzJd1h9khaRA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Pipeline with preprocessing + model\n",
    "full_pipeline = Pipeline([\n",
    "    # ('preprocessing', preprocessor),\n",
    "    ('clf', XGBClassifier(eval_metric='logloss', random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter search space for XGBoost (same ranges as grid search)\n",
    "param_distributions = {\n",
    "    'clf__n_estimators': [180, 200, 220, 250],\n",
    "    'clf__max_depth': [5, 6, 7, 9, 21],\n",
    "    'clf__learning_rate': [0.04, 0.05, 0.06, 0.07, 0.08],\n",
    "    'clf__subsample': [0.85, 0.9, 0.95],\n",
    "    'clf__colsample_bytree': [0.95, 1.0],\n",
    "    'clf__reg_alpha': [0, 0.01],\n",
    "    'clf__reg_lambda': [1, 1.2]\n",
    "}\n",
    "\n",
    "# Run hyperparameter optimization using RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=full_pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,                # Number of random samples (adjust based on computational budget)\n",
    "    scoring='accuracy',        # Main leaderboard metric\n",
    "    cv=5,                      # 5-fold cross-validation\n",
    "    n_jobs=-1,                 # Parallel execution\n",
    "    verbose=1,                 # Print progress\n",
    "    random_state=42,           # For reproducibility\n",
    "    return_train_score=True    # Track training performance\n",
    ")\n",
    "\n",
    "# Fit the pipeline to training data\n",
    "start_time = datetime.now()\n",
    "random_search.fit(X_train, y_train)\n",
    "end_time = datetime.now()\n",
    "\n",
    "# Print best result and CV performance\n",
    "print(\"üéØ Best Hyperparameters:\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"\\n‚úÖ Best CV Accuracy: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Extract all random search results for tracking\n",
    "cv_results = random_search.cv_results_\n",
    "results_list = []\n",
    "\n",
    "for i in range(len(cv_results['params'])):\n",
    "    # Extract hyperparameters\n",
    "    params = cv_results['params'][i]\n",
    "    clean_params = {k.replace('clf__', ''): v for k, v in params.items()}\n",
    "\n",
    "    # Get CV scores for each fold\n",
    "    cv_scores = []\n",
    "    for fold in range(5):\n",
    "        cv_scores.append(cv_results[f'split{fold}_test_score'][i])\n",
    "\n",
    "    # Create result record\n",
    "    result = {\n",
    "        'Model': 'XGBoost',\n",
    "        'Hyperparameters': str(clean_params),\n",
    "        'CV_Accuracy_Mean': cv_results['mean_test_score'][i],\n",
    "        'CV_Accuracy_Std': cv_results['std_test_score'][i],\n",
    "        'CV_Accuracy_Min': min(cv_scores),\n",
    "        'CV_Accuracy_Max': max(cv_scores),\n",
    "        'Rank': cv_results['rank_test_score'][i],\n",
    "        'Is_Best': i == random_search.best_index_,\n",
    "        'Runtime_Seconds': (end_time - start_time).total_seconds()\n",
    "    }\n",
    "\n",
    "    # Add individual hyperparameters as separate columns\n",
    "    for param_name, param_value in clean_params.items():\n",
    "        result[param_name] = param_value\n",
    "\n",
    "    results_list.append(result)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Display top 10 configurations\n",
    "print(\"\\nüìä TOP 10 CONFIGURATIONS:\")\n",
    "top_configs = results_df.nlargest(10, 'CV_Accuracy_Mean')[\n",
    "    ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'Hyperparameters', 'Rank']\n",
    "]\n",
    "print(top_configs.to_string(index=False))\n",
    "\n",
    "# Show accuracy range\n",
    "print(f\"\\nüìà PERFORMANCE SUMMARY:\")\n",
    "print(f\"Total experiments: {len(results_df)}\")\n",
    "print(f\"Best CV Accuracy: {results_df['CV_Accuracy_Mean'].max():.4f}\")\n",
    "print(f\"Worst CV Accuracy: {results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Accuracy Range: {results_df['CV_Accuracy_Mean'].max() - results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Total Runtime: {(end_time - start_time).total_seconds():.2f} seconds\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('results/xgboost_random_search_results.csv', index=False)\n",
    "print(f\"\\nüíæ Results saved to: results/xgboost_random_search_results.csv\")\n",
    "\n",
    "# Extract best model for reuse or export\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(best_model, 'best_xgb_random_model.pkl')\n",
    "print(\"‚úÖ Best model saved to: best_xgb_random_model.pkl\")\n",
    "\n",
    "# Display detailed results table\n",
    "print(f\"\\nüìã DETAILED RESULTS TABLE:\")\n",
    "display_cols = ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'n_estimators', 'max_depth', 'learning_rate', 'subsample', 'colsample_bytree', 'Rank']\n",
    "available_cols = [col for col in display_cols if col in results_df.columns]\n",
    "# results_df[available_cols].round(4).head(15)\n",
    "\n",
    "# Simple comparison note\n",
    "print(f\"\\nüí° Random Search vs Grid Search:\")\n",
    "print(f\"   ‚Ä¢ Random samples: {len(results_df)}\")\n",
    "print(f\"   ‚Ä¢ Grid combinations: {4*5*5*3*2*2*2} (would be ~2800)\")\n",
    "print(f\"   ‚Ä¢ Efficiency: ~{2800/len(results_df):.0f}x faster\")\n",
    "print(f\"   ‚Ä¢ Coverage: Random sampling across entire parameter space\")"
   ],
   "metadata": {
    "id": "-li9uOMFhajo",
    "ExecuteTime": {
     "end_time": "2025-06-06T19:40:44.012132Z",
     "start_time": "2025-06-06T19:40:28.170035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "üéØ Best Hyperparameters:\n",
      "{'clf__subsample': 0.95, 'clf__reg_lambda': 1, 'clf__reg_alpha': 0.01, 'clf__n_estimators': 180, 'clf__max_depth': 6, 'clf__learning_rate': 0.04, 'clf__colsample_bytree': 0.95}\n",
      "\n",
      "‚úÖ Best CV Accuracy: 0.7985\n",
      "\n",
      "üìä TOP 10 CONFIGURATIONS:\n",
      "  Model  CV_Accuracy_Mean  CV_Accuracy_Std                                                                                                                                Hyperparameters  Rank\n",
      "XGBoost          0.798461         0.011299  {'subsample': 0.95, 'reg_lambda': 1, 'reg_alpha': 0.01, 'n_estimators': 180, 'max_depth': 6, 'learning_rate': 0.04, 'colsample_bytree': 0.95}     1\n",
      "XGBoost          0.798461         0.008987 {'subsample': 0.85, 'reg_lambda': 1.2, 'reg_alpha': 0.01, 'n_estimators': 220, 'max_depth': 6, 'learning_rate': 0.05, 'colsample_bytree': 1.0}     2\n",
      "XGBoost          0.798347         0.011286     {'subsample': 0.9, 'reg_lambda': 1.2, 'reg_alpha': 0, 'n_estimators': 220, 'max_depth': 5, 'learning_rate': 0.07, 'colsample_bytree': 1.0}     3\n",
      "XGBoost          0.798231         0.008380    {'subsample': 0.85, 'reg_lambda': 1.2, 'reg_alpha': 0, 'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.05, 'colsample_bytree': 1.0}     4\n",
      "XGBoost          0.797886         0.011818     {'subsample': 0.95, 'reg_lambda': 1, 'reg_alpha': 0, 'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.08, 'colsample_bytree': 0.95}     5\n",
      "XGBoost          0.797886         0.010792   {'subsample': 0.95, 'reg_lambda': 1.2, 'reg_alpha': 0, 'n_estimators': 180, 'max_depth': 5, 'learning_rate': 0.06, 'colsample_bytree': 0.95}     6\n",
      "XGBoost          0.797886         0.009169  {'subsample': 0.9, 'reg_lambda': 1.2, 'reg_alpha': 0.01, 'n_estimators': 180, 'max_depth': 6, 'learning_rate': 0.07, 'colsample_bytree': 1.0}     7\n",
      "XGBoost          0.797772         0.012772  {'subsample': 0.95, 'reg_lambda': 1, 'reg_alpha': 0.01, 'n_estimators': 250, 'max_depth': 5, 'learning_rate': 0.04, 'colsample_bytree': 0.95}     8\n",
      "XGBoost          0.797771         0.010769   {'subsample': 0.9, 'reg_lambda': 1, 'reg_alpha': 0.01, 'n_estimators': 180, 'max_depth': 5, 'learning_rate': 0.05, 'colsample_bytree': 0.95}     9\n",
      "XGBoost          0.797771         0.010497   {'subsample': 0.95, 'reg_lambda': 1.2, 'reg_alpha': 0, 'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.07, 'colsample_bytree': 0.95}    10\n",
      "\n",
      "üìà PERFORMANCE SUMMARY:\n",
      "Total experiments: 100\n",
      "Best CV Accuracy: 0.7985\n",
      "Worst CV Accuracy: 0.7807\n",
      "Accuracy Range: 0.0177\n",
      "Total Runtime: 15.82 seconds\n",
      "\n",
      "üíæ Results saved to: random_search_results.csv\n",
      "‚úÖ Best model saved to: best_xgb_random_model.pkl\n",
      "\n",
      "üìã DETAILED RESULTS TABLE:\n",
      "\n",
      "üí° Random Search vs Grid Search:\n",
      "   ‚Ä¢ Random samples: 100\n",
      "   ‚Ä¢ Grid combinations: 2400 (would be ~2800)\n",
      "   ‚Ä¢ Efficiency: ~28x faster\n",
      "   ‚Ä¢ Coverage: Random sampling across entire parameter space\n",
      "CPU times: user 967 ms, sys: 824 ms, total: 1.79 s\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### STEP 2: Hyperparameter Tuning of CatBoost for Accuracy Optimization using Random Search\n",
    "This step fine-tunes the CatBoost model using RandomizedSearchCV with 5-fold cross-validation, targeting improved accuracy aligned with leaderboard evaluation. The objective is to efficiently identify high-performing configurations for deployment while maintaining generalizability and avoiding overfitting through stochastic hyperparameter exploration.\n",
    "\n",
    "Hyperparameter Tuning of CatBoost using Random Search, leveraging CatBoost's native categorical feature handling. This version includes:\n",
    "\n",
    "* Native categorical feature processing without preprocessing\n",
    "* Gradient boosting with advanced regularization techniques\n",
    "* Random search over CatBoost-specific hyperparameter distributions\n",
    "* Accuracy as the scoring metric\n",
    "* Efficient parameter space exploration through uniform random sampling\n",
    "* Advanced overfitting prevention through bagging temperature and random strength\n",
    "* Configurable number of iterations for computational budget control\n",
    "* Unbiased coverage of hyperparameter combinations without exhaustive enumeration"
   ],
   "metadata": {
    "id": "rbqQDNQqhRT5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from catboost import CatBoostClassifier\n",
    "import joblib\n",
    "\n",
    "# Pipeline with preprocessing + model\n",
    "full_pipeline = Pipeline([\n",
    "    # ('preprocessing', preprocessor),\n",
    "    ('clf', CatBoostClassifier(verbose=0, random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter search space for CatBoost (optimized for Spaceship Titanic)\n",
    "param_distributions = {\n",
    "    'clf__iterations': [500, 750, 1000, 1250, 1500],\n",
    "    'clf__depth': [4, 5, 6, 7, 8],\n",
    "    'clf__learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1],\n",
    "    'clf__l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "    'clf__border_count': [128, 254],\n",
    "    'clf__bagging_temperature': [0, 0.5, 1],\n",
    "    'clf__random_strength': [0, 1, 2]\n",
    "}\n",
    "\n",
    "# Run hyperparameter optimization using RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=full_pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,                # Number of random samples (adjust based on computational budget)\n",
    "    scoring='accuracy',        # Main leaderboard metric\n",
    "    cv=5,                      # 5-fold cross-validation\n",
    "    n_jobs=-1,                 # Parallel execution\n",
    "    verbose=1,                 # Print progress\n",
    "    random_state=42,           # For reproducibility\n",
    "    return_train_score=True    # Track training performance\n",
    ")\n",
    "\n",
    "# Fit the pipeline to training data\n",
    "start_time = datetime.now()\n",
    "random_search.fit(X_train, y_train)\n",
    "end_time = datetime.now()\n",
    "\n",
    "# Print best result and CV performance\n",
    "print(\"üéØ Best Hyperparameters:\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"\\n‚úÖ Best CV Accuracy: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Extract all random search results for tracking\n",
    "cv_results = random_search.cv_results_\n",
    "results_list = []\n",
    "\n",
    "for i in range(len(cv_results['params'])):\n",
    "    # Extract hyperparameters\n",
    "    params = cv_results['params'][i]\n",
    "    clean_params = {k.replace('clf__', ''): v for k, v in params.items()}\n",
    "\n",
    "    # Get CV scores for each fold\n",
    "    cv_scores = []\n",
    "    for fold in range(5):\n",
    "        cv_scores.append(cv_results[f'split{fold}_test_score'][i])\n",
    "\n",
    "    # Create result record\n",
    "    result = {\n",
    "        'Model': 'CatBoost',\n",
    "        'Hyperparameters': str(clean_params),\n",
    "        'CV_Accuracy_Mean': cv_results['mean_test_score'][i],\n",
    "        'CV_Accuracy_Std': cv_results['std_test_score'][i],\n",
    "        'CV_Accuracy_Min': min(cv_scores),\n",
    "        'CV_Accuracy_Max': max(cv_scores),\n",
    "        'Rank': cv_results['rank_test_score'][i],\n",
    "        'Is_Best': i == random_search.best_index_,\n",
    "        'Runtime_Seconds': (end_time - start_time).total_seconds()\n",
    "    }\n",
    "\n",
    "    # Add individual hyperparameters as separate columns\n",
    "    for param_name, param_value in clean_params.items():\n",
    "        result[param_name] = param_value\n",
    "\n",
    "    results_list.append(result)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Display top 10 configurations\n",
    "print(\"\\nüìä TOP 10 CONFIGURATIONS:\")\n",
    "top_configs = results_df.nlargest(10, 'CV_Accuracy_Mean')[\n",
    "    ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'Hyperparameters', 'Rank']\n",
    "]\n",
    "print(top_configs.to_string(index=False))\n",
    "\n",
    "# Show accuracy range\n",
    "print(f\"\\nüìà PERFORMANCE SUMMARY:\")\n",
    "print(f\"Total experiments: {len(results_df)}\")\n",
    "print(f\"Best CV Accuracy: {results_df['CV_Accuracy_Mean'].max():.4f}\")\n",
    "print(f\"Worst CV Accuracy: {results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Accuracy Range: {results_df['CV_Accuracy_Mean'].max() - results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Total Runtime: {(end_time - start_time).total_seconds():.2f} seconds\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('results/catboost_random_search_results.csv', index=False)\n",
    "print(f\"\\nüíæ Results saved to: results/catboost_random_search_results.csv\")\n",
    "\n",
    "# Extract best model for reuse or export\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(best_model, 'best_catboost_random_model.pkl')\n",
    "print(\"‚úÖ Best model saved to: best_catboost_random_model.pkl\")\n",
    "\n",
    "# Display detailed results table\n",
    "print(f\"\\nüìã DETAILED RESULTS TABLE:\")\n",
    "display_cols = ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'iterations', 'depth', 'learning_rate', 'l2_leaf_reg', 'bagging_temperature', 'Rank']\n",
    "available_cols = [col for col in display_cols if col in results_df.columns]\n",
    "# results_df[available_cols].round(4).head(15)\n",
    "\n",
    "# Simple comparison note\n",
    "print(f\"\\nüí° Random Search vs Grid Search:\")\n",
    "print(f\"   ‚Ä¢ Random samples: {len(results_df)}\")\n",
    "print(f\"   ‚Ä¢ Grid combinations: {5*5*5*5*2*3*3} (would be ~5625)\")\n",
    "print(f\"   ‚Ä¢ Efficiency: ~{5625/len(results_df):.0f}x faster\")\n",
    "print(f\"   ‚Ä¢ Coverage: Random sampling across entire parameter space\")"
   ],
   "metadata": {
    "id": "Cb6E9fAmhD1J",
    "ExecuteTime": {
     "end_time": "2025-06-06T19:43:40.318938Z",
     "start_time": "2025-06-06T19:40:44.047991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "üéØ Best Hyperparameters:\n",
      "{'clf__random_strength': 2, 'clf__learning_rate': 0.03, 'clf__l2_leaf_reg': 9, 'clf__iterations': 750, 'clf__depth': 4, 'clf__border_count': 128, 'clf__bagging_temperature': 0.5}\n",
      "\n",
      "‚úÖ Best CV Accuracy: 0.8009\n",
      "\n",
      "üìä TOP 10 CONFIGURATIONS:\n",
      "   Model  CV_Accuracy_Mean  CV_Accuracy_Std                                                                                                                                 Hyperparameters  Rank\n",
      "CatBoost          0.800878         0.013055 {'random_strength': 2, 'learning_rate': 0.03, 'l2_leaf_reg': 9, 'iterations': 750, 'depth': 4, 'border_count': 128, 'bagging_temperature': 0.5}     1\n",
      "CatBoost          0.800877         0.012536   {'random_strength': 0, 'learning_rate': 0.03, 'l2_leaf_reg': 7, 'iterations': 500, 'depth': 6, 'border_count': 254, 'bagging_temperature': 1}     2\n",
      "CatBoost          0.800876         0.008733 {'random_strength': 0, 'learning_rate': 0.03, 'l2_leaf_reg': 7, 'iterations': 750, 'depth': 7, 'border_count': 254, 'bagging_temperature': 0.5}     3\n",
      "CatBoost          0.800417         0.012072   {'random_strength': 0, 'learning_rate': 0.07, 'l2_leaf_reg': 3, 'iterations': 500, 'depth': 4, 'border_count': 254, 'bagging_temperature': 0}     4\n",
      "CatBoost          0.800302         0.012072   {'random_strength': 1, 'learning_rate': 0.03, 'l2_leaf_reg': 9, 'iterations': 500, 'depth': 4, 'border_count': 128, 'bagging_temperature': 0}     5\n",
      "CatBoost          0.799726         0.009300  {'random_strength': 0, 'learning_rate': 0.03, 'l2_leaf_reg': 5, 'iterations': 1000, 'depth': 5, 'border_count': 254, 'bagging_temperature': 0}     6\n",
      "CatBoost          0.799726         0.008534   {'random_strength': 1, 'learning_rate': 0.05, 'l2_leaf_reg': 3, 'iterations': 500, 'depth': 6, 'border_count': 254, 'bagging_temperature': 1}     7\n",
      "CatBoost          0.799381         0.007887   {'random_strength': 1, 'learning_rate': 0.03, 'l2_leaf_reg': 5, 'iterations': 750, 'depth': 6, 'border_count': 254, 'bagging_temperature': 0}     8\n",
      "CatBoost          0.799267         0.011806  {'random_strength': 0, 'learning_rate': 0.01, 'l2_leaf_reg': 5, 'iterations': 1000, 'depth': 4, 'border_count': 254, 'bagging_temperature': 1}     9\n",
      "CatBoost          0.799266         0.009872  {'random_strength': 1, 'learning_rate': 0.01, 'l2_leaf_reg': 7, 'iterations': 1250, 'depth': 6, 'border_count': 128, 'bagging_temperature': 0}    10\n",
      "\n",
      "üìà PERFORMANCE SUMMARY:\n",
      "Total experiments: 100\n",
      "Best CV Accuracy: 0.8009\n",
      "Worst CV Accuracy: 0.7848\n",
      "Accuracy Range: 0.0161\n",
      "Total Runtime: 176.25 seconds\n",
      "\n",
      "üíæ Results saved to: catboost_random_search_results.csv\n",
      "‚úÖ Best model saved to: best_catboost_random_model.pkl\n",
      "\n",
      "üìã DETAILED RESULTS TABLE:\n",
      "\n",
      "üí° Random Search vs Grid Search:\n",
      "   ‚Ä¢ Random samples: 100\n",
      "   ‚Ä¢ Grid combinations: 11250 (would be ~5625)\n",
      "   ‚Ä¢ Efficiency: ~56x faster\n",
      "   ‚Ä¢ Coverage: Random sampling across entire parameter space\n",
      "CPU times: user 5.72 s, sys: 3.14 s, total: 8.86 s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### STEP 3: Hyperparameter Tuning of Gradient Boosting for Accuracy Optimization using Random Search\n",
    "This step fine-tunes the Gradient Boosting model using RandomizedSearchCV with 5-fold cross-validation, targeting improved accuracy aligned with leaderboard evaluation. The objective is to efficiently identify high-performing configurations for deployment while maintaining generalizability and avoiding overfitting through stochastic hyperparameter exploration.\n",
    "\n",
    "Hyperparameter Tuning of Gradient Boosting using Random Search, utilizing scikit-learn's robust implementation. This version includes:\n",
    "\n",
    "* Sequential boosting with sample and feature subsampling\n",
    "* Tree-based weak learners with configurable depth and split criteria\n",
    "* Random search over gradient boosting hyperparameter distributions\n",
    "* Accuracy as the scoring metric\n",
    "* Efficient parameter space exploration through uniform random sampling\n",
    "* Overfitting control through subsample ratios and leaf constraints\n",
    "* Configurable number of iterations for computational budget control\n",
    "* Unbiased coverage of hyperparameter combinations without exhaustive enumeration"
   ],
   "metadata": {
    "id": "fQwThklJjryA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import joblib\n",
    "\n",
    "# Pipeline with preprocessing + model\n",
    "full_pipeline = Pipeline([\n",
    "    # ('preprocessing', preprocessor),\n",
    "    ('clf', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter search space for Gradient Boosting (optimized for Spaceship Titanic)\n",
    "param_distributions = {\n",
    "    'clf__n_estimators': [150, 200, 250, 300, 350],\n",
    "    'clf__max_depth': [3, 4, 5, 6, 7],\n",
    "    'clf__learning_rate': [0.05, 0.07, 0.1, 0.12, 0.15],\n",
    "    'clf__subsample': [0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    'clf__min_samples_split': [2, 5, 10, 15],\n",
    "    'clf__min_samples_leaf': [1, 2, 4, 6],\n",
    "    'clf__max_features': ['sqrt', 'log2', 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Run hyperparameter optimization using RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=full_pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,                # Number of random samples (adjust based on computational budget)\n",
    "    scoring='accuracy',        # Main leaderboard metric\n",
    "    cv=5,                      # 5-fold cross-validation\n",
    "    n_jobs=-1,                 # Parallel execution\n",
    "    verbose=1,                 # Print progress\n",
    "    random_state=42,           # For reproducibility\n",
    "    return_train_score=True    # Track training performance\n",
    ")\n",
    "\n",
    "# Fit the pipeline to training data\n",
    "start_time = datetime.now()\n",
    "random_search.fit(X_train, y_train)\n",
    "end_time = datetime.now()\n",
    "\n",
    "# Print best result and CV performance\n",
    "print(\"üéØ Best Hyperparameters:\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"\\n‚úÖ Best CV Accuracy: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Extract all random search results for tracking\n",
    "cv_results = random_search.cv_results_\n",
    "results_list = []\n",
    "\n",
    "for i in range(len(cv_results['params'])):\n",
    "    # Extract hyperparameters\n",
    "    params = cv_results['params'][i]\n",
    "    clean_params = {k.replace('clf__', ''): v for k, v in params.items()}\n",
    "\n",
    "    # Get CV scores for each fold\n",
    "    cv_scores = []\n",
    "    for fold in range(5):\n",
    "        cv_scores.append(cv_results[f'split{fold}_test_score'][i])\n",
    "\n",
    "    # Create result record\n",
    "    result = {\n",
    "        'Model': 'GradientBoosting',\n",
    "        'Hyperparameters': str(clean_params),\n",
    "        'CV_Accuracy_Mean': cv_results['mean_test_score'][i],\n",
    "        'CV_Accuracy_Std': cv_results['std_test_score'][i],\n",
    "        'CV_Accuracy_Min': min(cv_scores),\n",
    "        'CV_Accuracy_Max': max(cv_scores),\n",
    "        'Rank': cv_results['rank_test_score'][i],\n",
    "        'Is_Best': i == random_search.best_index_,\n",
    "        'Runtime_Seconds': (end_time - start_time).total_seconds()\n",
    "    }\n",
    "\n",
    "    # Add individual hyperparameters as separate columns\n",
    "    for param_name, param_value in clean_params.items():\n",
    "        result[param_name] = param_value\n",
    "\n",
    "    results_list.append(result)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Display top 10 configurations\n",
    "print(\"\\nüìä TOP 10 CONFIGURATIONS:\")\n",
    "top_configs = results_df.nlargest(10, 'CV_Accuracy_Mean')[\n",
    "    ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'Hyperparameters', 'Rank']\n",
    "]\n",
    "print(top_configs.to_string(index=False))\n",
    "\n",
    "# Show accuracy range\n",
    "print(f\"\\nüìà PERFORMANCE SUMMARY:\")\n",
    "print(f\"Total experiments: {len(results_df)}\")\n",
    "print(f\"Best CV Accuracy: {results_df['CV_Accuracy_Mean'].max():.4f}\")\n",
    "print(f\"Worst CV Accuracy: {results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Accuracy Range: {results_df['CV_Accuracy_Mean'].max() - results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Total Runtime: {(end_time - start_time).total_seconds():.2f} seconds\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('results/gradientboosting_random_search_results.csv', index=False)\n",
    "print(f\"\\nüíæ Results saved to: results/gradientboosting_random_search_results.csv\")\n",
    "\n",
    "# Extract best model for reuse or export\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(best_model, 'best_gradientboosting_random_model.pkl')\n",
    "print(\"‚úÖ Best model saved to: best_gradientboosting_random_model.pkl\")\n",
    "\n",
    "# Display detailed results table\n",
    "print(f\"\\nüìã DETAILED RESULTS TABLE:\")\n",
    "display_cols = ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'n_estimators', 'max_depth', 'learning_rate', 'subsample', 'min_samples_split', 'Rank']\n",
    "available_cols = [col for col in display_cols if col in results_df.columns]\n",
    "# results_df[available_cols].round(4).head(15)\n",
    "\n",
    "# Simple comparison note\n",
    "print(f\"\\nüí° Random Search vs Grid Search:\")\n",
    "print(f\"   ‚Ä¢ Random samples: {len(results_df)}\")\n",
    "print(f\"   ‚Ä¢ Grid combinations: {5*5*5*5*4*4*4} (would be ~8000)\")\n",
    "print(f\"   ‚Ä¢ Efficiency: ~{8000/len(results_df):.0f}x faster\")\n",
    "print(f\"   ‚Ä¢ Coverage: Random sampling across entire parameter space\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "t3VMvcwbz-BN",
    "outputId": "7ebaa947-c0b5-4638-b77e-df525fc77fc1",
    "ExecuteTime": {
     "end_time": "2025-06-06T19:44:30.307031Z",
     "start_time": "2025-06-06T19:43:40.382573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "üéØ Best Hyperparameters:\n",
      "{'clf__subsample': 0.9, 'clf__n_estimators': 300, 'clf__min_samples_split': 15, 'clf__min_samples_leaf': 6, 'clf__max_features': 'log2', 'clf__max_depth': 3, 'clf__learning_rate': 0.07}\n",
      "\n",
      "‚úÖ Best CV Accuracy: 0.7998\n",
      "\n",
      "üìä TOP 10 CONFIGURATIONS:\n",
      "           Model  CV_Accuracy_Mean  CV_Accuracy_Std                                                                                                                                         Hyperparameters  Rank\n",
      "GradientBoosting          0.799842         0.012974  {'subsample': 0.9, 'n_estimators': 300, 'min_samples_split': 15, 'min_samples_leaf': 6, 'max_features': 'log2', 'max_depth': 3, 'learning_rate': 0.07}     1\n",
      "GradientBoosting          0.799842         0.012974  {'subsample': 0.9, 'n_estimators': 300, 'min_samples_split': 15, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_depth': 3, 'learning_rate': 0.07}     1\n",
      "GradientBoosting          0.799037         0.012060  {'subsample': 0.95, 'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 3, 'learning_rate': 0.07}     3\n",
      "GradientBoosting          0.798577         0.012126 {'subsample': 0.95, 'n_estimators': 150, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 3, 'learning_rate': 0.12}     4\n",
      "GradientBoosting          0.797886         0.012214   {'subsample': 0.8, 'n_estimators': 150, 'min_samples_split': 2, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'max_depth': 3, 'learning_rate': 0.12}     5\n",
      "GradientBoosting          0.797885         0.007328   {'subsample': 0.9, 'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 5, 'learning_rate': 0.1}     6\n",
      "GradientBoosting          0.797772         0.012442 {'subsample': 0.85, 'n_estimators': 150, 'min_samples_split': 15, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 3, 'learning_rate': 0.12}     7\n",
      "GradientBoosting          0.797541         0.012209  {'subsample': 0.8, 'n_estimators': 250, 'min_samples_split': 15, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 4, 'learning_rate': 0.05}     8\n",
      "GradientBoosting          0.797426         0.011536     {'subsample': 0.9, 'n_estimators': 300, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 1.0, 'max_depth': 3, 'learning_rate': 0.05}     9\n",
      "GradientBoosting          0.797311         0.011421   {'subsample': 0.95, 'n_estimators': 350, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 3, 'learning_rate': 0.1}    10\n",
      "\n",
      "üìà PERFORMANCE SUMMARY:\n",
      "Total experiments: 100\n",
      "Best CV Accuracy: 0.7998\n",
      "Worst CV Accuracy: 0.7856\n",
      "Accuracy Range: 0.0143\n",
      "Total Runtime: 49.91 seconds\n",
      "\n",
      "üíæ Results saved to: gradientboosting_random_search_results.csv\n",
      "‚úÖ Best model saved to: best_gradientboosting_random_model.pkl\n",
      "\n",
      "üìã DETAILED RESULTS TABLE:\n",
      "\n",
      "üí° Random Search vs Grid Search:\n",
      "   ‚Ä¢ Random samples: 100\n",
      "   ‚Ä¢ Grid combinations: 40000 (would be ~8000)\n",
      "   ‚Ä¢ Efficiency: ~80x faster\n",
      "   ‚Ä¢ Coverage: Random sampling across entire parameter space\n",
      "CPU times: user 1.22 s, sys: 221 ms, total: 1.44 s\n",
      "Wall time: 49.9 s\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T19:48:11.670601Z",
     "start_time": "2025-06-06T19:48:11.646809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# SUMMARY: TOP PERFORMING MODELS COMPARISON\n",
    "# ================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ RANDOM SEARCH OPTIMIZATION SUMMARY - BEST SELECTED FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load results from each model\n",
    "xgb_results = pd.read_csv('../results/xgboost_random_search_results.csv')\n",
    "cat_results = pd.read_csv('../results/catboost_random_search_results.csv')\n",
    "gb_results = pd.read_csv('../results/gradientboosting_random_search_results.csv')\n",
    "\n",
    "# Extract best performance from each model\n",
    "summary_data = []\n",
    "\n",
    "# XGBoost best\n",
    "xgb_best = xgb_results.loc[xgb_results['CV_Accuracy_Mean'].idxmax()]\n",
    "summary_data.append({\n",
    "    'Model': 'XGBoost',\n",
    "    'CV_Accuracy_Mean': xgb_best['CV_Accuracy_Mean'],\n",
    "    'CV_Accuracy_Std': xgb_best['CV_Accuracy_Std']\n",
    "})\n",
    "\n",
    "# CatBoost best\n",
    "cat_best = cat_results.loc[cat_results['CV_Accuracy_Mean'].idxmax()]\n",
    "summary_data.append({\n",
    "    'Model': 'CatBoost',\n",
    "    'CV_Accuracy_Mean': cat_best['CV_Accuracy_Mean'],\n",
    "    'CV_Accuracy_Std': cat_best['CV_Accuracy_Std']\n",
    "})\n",
    "\n",
    "# Gradient Boosting best\n",
    "gb_best = gb_results.loc[gb_results['CV_Accuracy_Mean'].idxmax()]\n",
    "summary_data.append({\n",
    "    'Model': 'GradientBoosting',\n",
    "    'CV_Accuracy_Mean': gb_best['CV_Accuracy_Mean'],\n",
    "    'CV_Accuracy_Std': gb_best['CV_Accuracy_Std']\n",
    "})\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(summary_data).sort_values('CV_Accuracy_Mean', ascending=False)\n",
    "\n",
    "# Display summary\n",
    "print(\"üìä BEST PERFORMANCE FROM EACH MODEL:\")\n",
    "print(summary_df.round(4).to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv('results/random_search_summary.csv', index=False)\n",
    "print(f\"\\nüíæ Summary saved to: results/random_search_summary.csv\")\n",
    "\n",
    "# Winner announcement\n",
    "winner = summary_df.iloc[0]\n",
    "print(f\"\\nüèÜ RANDOM SEARCH WINNER: {winner['Model']}\")\n",
    "print(f\"üèÜ Best CV Accuracy: {winner['CV_Accuracy_Mean']:.4f} (¬±{winner['CV_Accuracy_Std']:.4f})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üèÜ RANDOM SEARCH OPTIMIZATION SUMMARY - BEST SELECTED FEATURES\n",
      "============================================================\n",
      "üìä BEST PERFORMANCE FROM EACH MODEL:\n",
      "           Model  CV_Accuracy_Mean  CV_Accuracy_Std\n",
      "        CatBoost            0.8009           0.0131\n",
      "GradientBoosting            0.7998           0.0130\n",
      "         XGBoost            0.7985           0.0113\n",
      "\n",
      "üíæ Summary saved to: random_search_summary.csv\n",
      "\n",
      "üèÜ RANDOM SEARCH WINNER: CatBoost\n",
      "üèÜ Best CV Accuracy: 0.8009 (¬±0.0131)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
