{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 100411,
     "databundleVersionId": 12064814,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  },
  "colab": {
   "name": "Template Notebook",
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat_minor": 0,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/stepthom/869_course/blob/main/2026%20869%20Project%20Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MMAI 869 Project: Example Notebook\n",
    "\n",
    "*Updated May 1, 2025*\n",
    "\n",
    "This notebook serves as a template for the Team Project. Teams can use this notebook as a starting point, and update it successively with new ideas and techniques to improve their model results.\n",
    "\n",
    "Note that is not required to use this template. Teams may also alter this template in any way they see fit."
   ],
   "metadata": {
    "id": "T_JqF4nhnHAK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preliminaries: Inspect and Set up environment\n",
    "\n",
    "No action is required on your part in this section. These cells print out helpful information about the environment, just in case."
   ],
   "metadata": {
    "id": "5h8kN7e4nHAM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# üß∞ General-purpose libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "\n",
    "# üß™ Scikit-learn preprocessing & pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# üîç Scikit-learn model selection\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold\n",
    ")\n",
    "\n",
    "# üß† Scikit-learn classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# üöÄ Gradient boosting frameworks\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# üìä Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# üß™ Sample dataset (for testing/demo)\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-29T15:33:14.024124Z",
     "iopub.execute_input": "2025-04-29T15:33:14.024385Z",
     "iopub.status.idle": "2025-04-29T15:33:23.963717Z",
     "shell.execute_reply.started": "2025-04-29T15:33:14.024359Z",
     "shell.execute_reply": "2025-04-29T15:33:23.962752Z"
    },
    "id": "iFGZKq7dnHAN",
    "ExecuteTime": {
     "end_time": "2025-06-06T18:27:47.213028Z",
     "start_time": "2025-06-06T18:27:46.782952Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "!python --version"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-29T15:34:57.648204Z",
     "iopub.execute_input": "2025-04-29T15:34:57.64854Z",
     "iopub.status.idle": "2025-04-29T15:34:57.787301Z",
     "shell.execute_reply.started": "2025-04-29T15:34:57.648514Z",
     "shell.execute_reply": "2025-04-29T15:34:57.786221Z"
    },
    "id": "M8P3CPDmnHAP",
    "ExecuteTime": {
     "end_time": "2025-06-06T18:27:47.473019Z",
     "start_time": "2025-06-06T18:27:47.339291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.16\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0: Data Loading and Inspection"
   ],
   "metadata": {
    "id": "c5EsSXF-nHAQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load dataset for the project\n",
    "df_train = pd.read_csv(\"data_yun/X_train.csv\")\n",
    "df_test = pd.read_csv(\"data_yun/X_test.csv\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-29T15:41:49.690048Z",
     "iopub.execute_input": "2025-04-29T15:41:49.690355Z",
     "iopub.status.idle": "2025-04-29T15:41:50.067303Z",
     "shell.execute_reply.started": "2025-04-29T15:41:49.690336Z",
     "shell.execute_reply": "2025-04-29T15:41:50.066321Z"
    },
    "id": "t9bKTR3fnHAQ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's print some descriptive statistics for all the numeric features.\n",
    "df_train.describe().T"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-29T15:42:47.74599Z",
     "iopub.execute_input": "2025-04-29T15:42:47.746294Z",
     "iopub.status.idle": "2025-04-29T15:42:47.778172Z",
     "shell.execute_reply.started": "2025-04-29T15:42:47.746272Z",
     "shell.execute_reply": "2025-04-29T15:42:47.777304Z"
    },
    "id": "qH-m99m1nHAS"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# How much missing data is in each feature?\n",
    "df_train.isna().sum()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-29T15:46:20.628514Z",
     "iopub.execute_input": "2025-04-29T15:46:20.62892Z",
     "iopub.status.idle": "2025-04-29T15:46:20.644894Z",
     "shell.execute_reply.started": "2025-04-29T15:46:20.628893Z",
     "shell.execute_reply": "2025-04-29T15:46:20.644036Z"
    },
    "id": "Ds-GenRSnHAT"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-29T16:05:54.011414Z",
     "iopub.execute_input": "2025-04-29T16:05:54.012163Z",
     "iopub.status.idle": "2025-04-29T16:05:54.018336Z",
     "shell.execute_reply.started": "2025-04-29T16:05:54.012134Z",
     "shell.execute_reply": "2025-04-29T16:05:54.017386Z"
    },
    "id": "NrgbYV13nHAV"
   },
   "cell_type": "code",
   "source": [
    "# Scikit-learn needs us to put the features in one dataframe, and the label in another.\n",
    "# It's tradition to name these variables X and y, but it doesn't really matter.\n",
    "\n",
    "X_train = df_train.drop(['Transported'], axis=1)\n",
    "y_train = df_train['Transported']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2: Model creation, hyperparameter tuning, and validation"
   ],
   "metadata": {
    "id": "MQGrMwxMnHAX"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### STEP 2: Hyperparameter Tuning of XGBoost for Accuracy Optimization\n",
    "This step fine-tunes the XGBoost model using GridSearchCV with 5-fold cross-validation, targeting improved accuracy aligned with leaderboard evaluation. The objective is to identify the best-performing configuration for deployment while maintaining generalizability and avoiding overfitting.\n",
    "\n",
    "Hyperparameter Tuning of XGBoost, includes feature engineering integrated into a pipeline using ColumnTransformer. This version includes:\n",
    "- Imputation and scaling for numeric features\n",
    "- Imputation and one-hot encoding for categorical features\n",
    "- Modular pipeline with XGBoost\n",
    "- Grid search over relevant hyperparameters\n",
    "- Accuracy as the scoring metric"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "# Pipeline with preprocessing + model\n",
    "full_pipeline = Pipeline([\n",
    "    # ('preprocessing', preprocessor),\n",
    "    ('clf', XGBClassifier(eval_metric='logloss', random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter search space for XGBoost\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [180, 200, 220, 250],\n",
    "    'clf__max_depth': [5, 6, 7, 9, 21],\n",
    "    'clf__learning_rate': [0.04, 0.05, 0.06, 0.07, 0.08],\n",
    "    'clf__subsample': [0.85, 0.9, 0.95],\n",
    "    'clf__colsample_bytree': [0.95, 1.0],\n",
    "    'clf__reg_alpha': [0, 0.01],\n",
    "    'clf__reg_lambda': [1, 1.2]\n",
    "}\n",
    "\n",
    "# Run hyperparameter optimization using GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=full_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',        # Main leaderboard metric\n",
    "    cv=5,                      # 5-fold cross-validation\n",
    "    n_jobs=-1,                 # Parallel execution\n",
    "    verbose=1,                 # Print progress\n",
    "    return_train_score=True    # Track training performance\n",
    ")\n",
    "\n",
    "# Fit the pipeline to training data\n",
    "start_time = datetime.now()\n",
    "grid_search.fit(X_train, y_train)\n",
    "end_time = datetime.now()\n",
    "\n",
    "# Print best result and CV performance\n",
    "print(\"üéØ Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"\\n‚úÖ Best CV Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Extract all grid search results for tracking\n",
    "cv_results = grid_search.cv_results_\n",
    "results_list = []\n",
    "\n",
    "for i in range(len(cv_results['params'])):\n",
    "    # Extract hyperparameters\n",
    "    params = cv_results['params'][i]\n",
    "    clean_params = {k.replace('clf__', ''): v for k, v in params.items()}\n",
    "\n",
    "    # Get CV scores for each fold\n",
    "    cv_scores = []\n",
    "    for fold in range(5):\n",
    "        cv_scores.append(cv_results[f'split{fold}_test_score'][i])\n",
    "\n",
    "    # Create result record\n",
    "    result = {\n",
    "        'Model': 'XGBoost',\n",
    "        'Hyperparameters': str(clean_params),\n",
    "        'CV_Accuracy_Mean': cv_results['mean_test_score'][i],\n",
    "        'CV_Accuracy_Std': cv_results['std_test_score'][i],\n",
    "        'CV_Accuracy_Min': min(cv_scores),\n",
    "        'CV_Accuracy_Max': max(cv_scores),\n",
    "        'Rank': cv_results['rank_test_score'][i],\n",
    "        'Is_Best': i == grid_search.best_index_,\n",
    "        'Runtime_Seconds': (end_time - start_time).total_seconds()\n",
    "    }\n",
    "\n",
    "    # Add individual hyperparameters as separate columns\n",
    "    for param_name, param_value in clean_params.items():\n",
    "        result[f'{param_name}'] = param_value\n",
    "\n",
    "    results_list.append(result)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Display top 10 configurations\n",
    "print(\"\\nüìä TOP 10 CONFIGURATIONS:\")\n",
    "top_configs = results_df.nlargest(10, 'CV_Accuracy_Mean')[\n",
    "    ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'Hyperparameters', 'Rank']\n",
    "]\n",
    "print(top_configs.to_string(index=False))\n",
    "\n",
    "# Show accuracy range\n",
    "print(f\"\\nüìà PERFORMANCE SUMMARY:\")\n",
    "print(f\"Total experiments: {len(results_df)}\")\n",
    "print(f\"Best CV Accuracy: {results_df['CV_Accuracy_Mean'].max():.4f}\")\n",
    "print(f\"Worst CV Accuracy: {results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Accuracy Range: {results_df['CV_Accuracy_Mean'].max() - results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Total Runtime: {(end_time - start_time).total_seconds():.2f} seconds\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('grid_search_results.csv', index=False)\n",
    "print(f\"\\nüíæ Results saved to: grid_search_results.csv\")\n",
    "\n",
    "# Extract best model for reuse or export\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(best_model, 'best_xgb_model.pkl')\n",
    "print(\"‚úÖ Best model saved to: best_xgb_model.pkl\")\n",
    "\n",
    "# Display detailed results table\n",
    "print(f\"\\nüìã DETAILED RESULTS TABLE:\")\n",
    "display_cols = ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'n_estimators', 'max_depth', 'learning_rate', 'subsample', 'colsample_bytree', 'Rank']\n",
    "print(results_df[display_cols].round(4).to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### STEP 3: Hyperparameter Tuning of RandomForest for Accuracy Optimization\n",
    "This step fine-tunes the RandomForest model using GridSearchCV with 5-fold cross-validation, targeting improved accuracy aligned with leaderboard evaluation. The objective is to identify the best-performing configuration for deployment while maintaining generalizability and avoiding overfitting.\n",
    "\n",
    "Hyperparameter Tuning of RandomForest includes feature engineering integrated into a pipeline using ColumnTransformer. This version includes:\n",
    "- Imputation and scaling for numeric features\n",
    "- Imputation and one-hot encoding for categorical features\n",
    "- Modular pipeline with RandomForestClassifier\n",
    "- Grid search over relevant hyperparameters optimized for mixed data types\n",
    "- Accuracy as the scoring metric\n",
    "- Bootstrap and class weighting strategies for handling potential imbalanced spaceship passenger data\n",
    "- Tree-specific parameters (depth, split criteria, leaf constraints) tailored for categorical feature handling\n",
    "\n",
    "1620 candidates represents all possible hyperparameter combinations being tested, and with 5-fold cross-validation, each combination is evaluated 5 times on different data splits, resulting in 8100 total model training runs to find the optimal RandomForest configuration."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "# Pipeline with preprocessing + model\n",
    "full_pipeline = Pipeline([\n",
    "    # ('preprocessing', preprocessor),\n",
    "    ('clf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Research-based hyperparameter search space for RandomForest (optimized for mixed data types)\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [100, 120, 150],                   # Number of trees - more trees generally better\n",
    "    'clf__max_depth': [None, 25, 30],                       # Tree depth - None allows full growth\n",
    "    'clf__min_samples_split': [2, 3, 4],                    # Minimum samples to split internal node\n",
    "    'clf__min_samples_leaf': [1, 2],                        # Minimum samples at leaf node\n",
    "    'clf__max_features': ['sqrt', 'log2'],                  # Features to consider at each split\n",
    "    'clf__bootstrap': [True],                               # Whether to use bootstrap samples\n",
    "    'clf__class_weight': [None, 'balanced']                 # Handle potential class imbalance\n",
    "}\n",
    "\n",
    "# Run hyperparameter optimization using GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=full_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',        # Main leaderboard metric\n",
    "    cv=5,                      # 5-fold cross-validation\n",
    "    n_jobs=-1,                 # Parallel execution\n",
    "    verbose=1,                 # Print progress\n",
    "    return_train_score=True    # Track training performance\n",
    ")\n",
    "\n",
    "# Fit the pipeline to training data\n",
    "start_time = datetime.now()\n",
    "grid_search.fit(X_train, y_train)\n",
    "end_time = datetime.now()\n",
    "\n",
    "# Print best result and CV performance\n",
    "print(\"üéØ Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"\\n‚úÖ Best CV Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Extract all grid search results for tracking\n",
    "cv_results = grid_search.cv_results_\n",
    "results_list = []\n",
    "\n",
    "for i in range(len(cv_results['params'])):\n",
    "    # Extract hyperparameters\n",
    "    params = cv_results['params'][i]\n",
    "    clean_params = {k.replace('clf__', ''): v for k, v in params.items()}\n",
    "\n",
    "    # Get CV scores for each fold\n",
    "    cv_scores = []\n",
    "    for fold in range(5):\n",
    "        cv_scores.append(cv_results[f'split{fold}_test_score'][i])\n",
    "\n",
    "    # Create result record\n",
    "    result = {\n",
    "        'Model': 'RandomForest',\n",
    "        'Hyperparameters': str(clean_params),\n",
    "        'CV_Accuracy_Mean': cv_results['mean_test_score'][i],\n",
    "        'CV_Accuracy_Std': cv_results['std_test_score'][i],\n",
    "        'CV_Accuracy_Min': min(cv_scores),\n",
    "        'CV_Accuracy_Max': max(cv_scores),\n",
    "        'Rank': cv_results['rank_test_score'][i],\n",
    "        'Is_Best': i == grid_search.best_index_,\n",
    "        'Runtime_Seconds': (end_time - start_time).total_seconds()\n",
    "    }\n",
    "\n",
    "    # Add individual hyperparameters as separate columns\n",
    "    for param_name, param_value in clean_params.items():\n",
    "        result[f'{param_name}'] = param_value\n",
    "\n",
    "    results_list.append(result)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Display top 10 configurations\n",
    "print(\"\\nüìä TOP 10 CONFIGURATIONS:\")\n",
    "top_configs = results_df.nlargest(10, 'CV_Accuracy_Mean')[\n",
    "    ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'Hyperparameters', 'Rank']\n",
    "]\n",
    "print(top_configs.to_string(index=False))\n",
    "\n",
    "# Show accuracy range\n",
    "print(f\"\\nüìà PERFORMANCE SUMMARY:\")\n",
    "print(f\"Total experiments: {len(results_df)}\")\n",
    "print(f\"Best CV Accuracy: {results_df['CV_Accuracy_Mean'].max():.4f}\")\n",
    "print(f\"Worst CV Accuracy: {results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Accuracy Range: {results_df['CV_Accuracy_Mean'].max() - results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Total Runtime: {(end_time - start_time).total_seconds():.2f} seconds\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('grid_search_results.csv', index=False)\n",
    "print(f\"\\nüíæ Results saved to: grid_search_results.csv\")\n",
    "\n",
    "# Extract best model for reuse or export\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(best_model, 'best_randomforest_model.pkl')\n",
    "print(\"‚úÖ Best model saved to: best_randomforest_model.pkl\")\n",
    "\n",
    "# Display detailed results table\n",
    "print(f\"\\nüìã DETAILED RESULTS TABLE:\")\n",
    "display_cols = ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'max_features', 'bootstrap', 'class_weight', 'Rank']\n",
    "print(results_df[display_cols].round(4).to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### STEP 4: Hyperparameter Tuning of LogisticRegression for Accuracy Optimization\n",
    "This step fine-tunes the LogisticRegression model using GridSearchCV with 5-fold cross-validation, targeting improved accuracy aligned with leaderboard evaluation. The objective is to identify the best-performing configuration for deployment while maintaining generalizability and avoiding overfitting.\n",
    "\n",
    "Hyperparameter Tuning of LogisticRegression includes feature engineering integrated into a pipeline using ColumnTransformer. This version includes:\n",
    "- Imputation and scaling for numeric features\n",
    "- Imputation and one-hot encoding for categorical features\n",
    "- Modular pipeline with LogisticRegression\n",
    "- Grid search over relevant hyperparameters optimized for high-dimensional encoded data\n",
    "- Accuracy as the scoring metric\n",
    "- Regularization strategies (L1, L2, ElasticNet) for feature selection and overfitting prevention\n",
    "- Class weighting for handling potential imbalanced spaceship passenger data\n",
    "- Solver optimization for convergence with mixed categorical/numerical features\n",
    "\n",
    "240 candidates represents all possible hyperparameter combinations being tested, and with 5-fold cross-validation, each combination is evaluated 5 times on different data splits, resulting in 1200 total model training runs to find the optimal LogisticRegression configuration."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "\n",
    "# Pipeline with preprocessing + model\n",
    "full_pipeline = Pipeline([\n",
    "    # ('preprocessing', preprocessor),\n",
    "    ('clf', LogisticRegression(random_state=42, max_iter=2000))\n",
    "])\n",
    "\n",
    "# Research-based hyperparameter search space for LogisticRegression (optimized for mixed data types)\n",
    "param_grid = {\n",
    "    'clf__C': [0.5, 1.0, 2.0, 5.0],                        # Regularization strength (inverse)\n",
    "    'clf__penalty': ['l2'],                                 # Regularization type\n",
    "    'clf__solver': ['lbfgs'],                               # Optimization algorithm\n",
    "    'clf__class_weight': [None, 'balanced'],                # Handle potential class imbalance\n",
    "    'clf__fit_intercept': [True],                           # Whether to fit intercept term\n",
    "    'clf__max_iter': [2000]                                 # Maximum iterations for convergence\n",
    "}\n",
    "\n",
    "# Run hyperparameter optimization using GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=full_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',        # Main leaderboard metric\n",
    "    cv=5,                      # 5-fold cross-validation\n",
    "    n_jobs=-1,                 # Parallel execution\n",
    "    verbose=1,                 # Print progress\n",
    "    return_train_score=True    # Track training performance\n",
    ")\n",
    "\n",
    "# Fit the pipeline to training data\n",
    "start_time = datetime.now()\n",
    "grid_search.fit(X_train, y_train)\n",
    "end_time = datetime.now()\n",
    "\n",
    "# Print best result and CV performance\n",
    "print(\"üéØ Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"\\n‚úÖ Best CV Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Extract all grid search results for tracking\n",
    "cv_results = grid_search.cv_results_\n",
    "results_list = []\n",
    "\n",
    "for i in range(len(cv_results['params'])):\n",
    "    # Extract hyperparameters\n",
    "    params = cv_results['params'][i]\n",
    "    clean_params = {k.replace('clf__', ''): v for k, v in params.items()}\n",
    "\n",
    "    # Get CV scores for each fold\n",
    "    cv_scores = []\n",
    "    for fold in range(5):\n",
    "        cv_scores.append(cv_results[f'split{fold}_test_score'][i])\n",
    "\n",
    "    # Create result record\n",
    "    result = {\n",
    "        'Model': 'LogisticRegression',\n",
    "        'Hyperparameters': str(clean_params),\n",
    "        'CV_Accuracy_Mean': cv_results['mean_test_score'][i],\n",
    "        'CV_Accuracy_Std': cv_results['std_test_score'][i],\n",
    "        'CV_Accuracy_Min': min(cv_scores),\n",
    "        'CV_Accuracy_Max': max(cv_scores),\n",
    "        'Rank': cv_results['rank_test_score'][i],\n",
    "        'Is_Best': i == grid_search.best_index_,\n",
    "        'Runtime_Seconds': (end_time - start_time).total_seconds()\n",
    "    }\n",
    "\n",
    "    # Add individual hyperparameters as separate columns\n",
    "    for param_name, param_value in clean_params.items():\n",
    "        result[param_name] = param_value\n",
    "\n",
    "    results_list.append(result)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Display top 10 configurations\n",
    "print(\"\\nüìä TOP 10 CONFIGURATIONS:\")\n",
    "top_configs = results_df.nlargest(10, 'CV_Accuracy_Mean')[\n",
    "    ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'Hyperparameters', 'Rank']\n",
    "]\n",
    "print(top_configs.to_string(index=False))\n",
    "\n",
    "# Show accuracy range\n",
    "print(f\"\\nüìà PERFORMANCE SUMMARY:\")\n",
    "print(f\"Total experiments: {len(results_df)}\")\n",
    "print(f\"Best CV Accuracy: {results_df['CV_Accuracy_Mean'].max():.4f}\")\n",
    "print(f\"Worst CV Accuracy: {results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Accuracy Range: {results_df['CV_Accuracy_Mean'].max() - results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Total Runtime: {(end_time - start_time).total_seconds():.2f} seconds\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('logistic_regression_grid_search_results.csv', index=False)\n",
    "print(f\"\\nüíæ Results saved to: logistic_regression_grid_search_results.csv\")\n",
    "\n",
    "# Extract best model for reuse or export\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(best_model, 'best_logisticregression_model.pkl')\n",
    "print(\"‚úÖ Best model saved to: best_logisticregression_model.pkl\")\n",
    "\n",
    "# Display detailed results table\n",
    "print(f\"\\nüìã DETAILED RESULTS TABLE:\")\n",
    "display_cols = ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'C', 'penalty', 'solver', 'class_weight', 'fit_intercept', 'max_iter', 'Rank']\n",
    "available_cols = [col for col in display_cols if col in results_df.columns]\n",
    "print(results_df[available_cols].round(4).to_string(index=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### STEP 5: Grid Search for model selection among XGBoost, RandomForest, and LogisticRegression"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "# =============================================================================\n",
    "# DEFINE YOUR BEST MODELS (Replace with your actual best hyperparameters)\n",
    "# =============================================================================\n",
    "print(\"\\nüéØ Configuring Best Models from Previous Grid Searches\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# üéØ Best Hyperparameters: {'clf__colsample_bytree': 0.7, 'clf__learning_rate': 0.1, 'clf__max_depth': 7, 'clf__n_estimators': 100, 'clf__subsample': 0.9}\n",
    "best_models = {\n",
    "    'XGBoost_Best': XGBClassifier(\n",
    "        n_estimators=100,           # Your best value\n",
    "        max_depth=7,                # Your best value\n",
    "        learning_rate=0.1,          # Your best value\n",
    "        subsample=0.9,              # Your best value\n",
    "        colsample_bytree=0.7,       # Your best value\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    'RandomForest_Best': RandomForestClassifier(\n",
    "        n_estimators=100,           # Your best value (not 300)\n",
    "        max_depth=None,             # Your best value (unlimited depth)\n",
    "        min_samples_split=4,        # Your best value (not 2)\n",
    "        min_samples_leaf=1,         # Your best value\n",
    "        max_features='sqrt',        # Your best value\n",
    "        bootstrap=True,             # Your best value\n",
    "        class_weight='balanced',    # Your best value (not None)\n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    'LogisticRegression_Best': LogisticRegression(\n",
    "        C=0.5,                      # Your best value (not 1.0)\n",
    "        penalty='l2',               # Your best value\n",
    "        solver='lbfgs',             # Your best value (not liblinear)\n",
    "        class_weight=None,          # Your best value\n",
    "        fit_intercept=True,         # Your best value\n",
    "        max_iter=2000,              # Your best value (not 1000)\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Best models configured from previous grid search results\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMPREHENSIVE COMPARISON OF BEST MODELS\n",
    "# =============================================================================\n",
    "print(\"\\nüîç Comprehensive Evaluation of Best Models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model in best_models.items():\n",
    "    print(f\"\\nüöÄ Evaluating: {model_name}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        # ('preprocessing', preprocessor),\n",
    "        ('clf', model)\n",
    "    ])\n",
    "\n",
    "    # Perform comprehensive cross-validation\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    cv_results = cross_validate(\n",
    "        pipeline,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        scoring=['accuracy', 'f1', 'f1_macro', 'roc_auc', 'precision', 'recall'],\n",
    "        return_train_score=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    runtime = (end_time - start_time).total_seconds()\n",
    "\n",
    "    # Calculate comprehensive metrics\n",
    "    result = {\n",
    "        'Model_Name': model_name,\n",
    "        'CV_Accuracy': cv_results['test_accuracy'].mean(),\n",
    "        'CV_Accuracy_Std': cv_results['test_accuracy'].std(),\n",
    "        'Train_Accuracy': cv_results['train_accuracy'].mean(),\n",
    "        'CV_F1': cv_results['test_f1'].mean(),\n",
    "        'CV_F1_Std': cv_results['test_f1'].std(),\n",
    "        'CV_F1_Macro': cv_results['test_f1_macro'].mean(),\n",
    "        'CV_Precision': cv_results['test_precision'].mean(),\n",
    "        'CV_Recall': cv_results['test_recall'].mean(),\n",
    "        'CV_AUC': cv_results['test_roc_auc'].mean(),\n",
    "        'CV_AUC_Std': cv_results['test_roc_auc'].std(),\n",
    "        'Runtime_Seconds': runtime,\n",
    "        'Overfitting_Gap': cv_results['train_accuracy'].mean() - cv_results['test_accuracy'].mean()\n",
    "    }\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"‚úÖ Results for {model_name}:\")\n",
    "    print(f\"   CV Accuracy:  {result['CV_Accuracy']:.4f} (¬±{result['CV_Accuracy_Std']:.4f})\")\n",
    "    print(f\"   CV F1:        {result['CV_F1']:.4f} (¬±{result['CV_F1_Std']:.4f})\")\n",
    "    print(f\"   CV AUC:       {result['CV_AUC']:.4f} (¬±{result['CV_AUC_Std']:.4f})\")\n",
    "    print(f\"   CV Precision: {result['CV_Precision']:.4f}\")\n",
    "    print(f\"   CV Recall:    {result['CV_Recall']:.4f}\")\n",
    "    print(f\"   Runtime:      {runtime:.2f} seconds\")\n",
    "    print(f\"   Overfitting:  {result['Overfitting_Gap']:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL ANALYSIS AND SELECTION\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä FINAL ANALYSIS AND MODEL SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Identify the ultimate winner\n",
    "best_idx = results_df['CV_Accuracy'].idxmax()\n",
    "best_model_result = results_df.loc[best_idx]\n",
    "\n",
    "print(f\"üèÜ ULTIMATE WINNER: {best_model_result['Model_Name']}\")\n",
    "print(f\"   CV Accuracy: {best_model_result['CV_Accuracy']:.4f} (¬±{best_model_result['CV_Accuracy_Std']:.4f})\")\n",
    "print(f\"   CV F1:       {best_model_result['CV_F1']:.4f}\")\n",
    "print(f\"   CV AUC:      {best_model_result['CV_AUC']:.4f}\")\n",
    "print(f\"   Runtime:     {best_model_result['Runtime_Seconds']:.2f} seconds\")\n",
    "\n",
    "# Comprehensive comparison table\n",
    "print(f\"\\nüìã COMPREHENSIVE COMPARISON TABLE:\")\n",
    "display_cols = ['Model_Name', 'CV_Accuracy', 'CV_F1', 'CV_AUC', 'CV_Precision', 'CV_Recall', 'Runtime_Seconds', 'Overfitting_Gap']\n",
    "comparison_df = results_df[display_cols].round(4).sort_values('CV_Accuracy', ascending=False)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Performance rankings\n",
    "print(f\"\\nüèÖ PERFORMANCE RANKINGS:\")\n",
    "rankings = {\n",
    "    'By Accuracy': results_df.sort_values('CV_Accuracy', ascending=False)['Model_Name'].tolist(),\n",
    "    'By F1 Score': results_df.sort_values('CV_F1', ascending=False)['Model_Name'].tolist(),\n",
    "    'By AUC': results_df.sort_values('CV_AUC', ascending=False)['Model_Name'].tolist(),\n",
    "    'By Speed': results_df.sort_values('Runtime_Seconds', ascending=True)['Model_Name'].tolist(),\n",
    "    'By Stability': results_df.sort_values('Overfitting_Gap', ascending=True)['Model_Name'].tolist()\n",
    "}\n",
    "\n",
    "for metric, ranking in rankings.items():\n",
    "    print(f\"   {metric:<15}: {' > '.join(ranking)}\")\n",
    "\n",
    "# Statistical significance analysis\n",
    "print(f\"\\nüìà STATISTICAL SIGNIFICANCE ANALYSIS:\")\n",
    "accuracy_std = results_df['CV_Accuracy_Std']\n",
    "best_accuracy = results_df['CV_Accuracy'].max()\n",
    "second_best_accuracy = results_df['CV_Accuracy'].nlargest(2).iloc[1]\n",
    "accuracy_diff = best_accuracy - second_best_accuracy\n",
    "\n",
    "print(f\"   Best accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"   2nd best:      {second_best_accuracy:.4f}\")\n",
    "print(f\"   Difference:    {accuracy_diff:.4f}\")\n",
    "print(f\"   Significant:   {'Yes' if accuracy_diff > 0.01 else 'Marginal' if accuracy_diff > 0.005 else 'No'}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE FINAL RESULTS AND BEST MODEL\n",
    "# =============================================================================\n",
    "print(f\"\\nüíæ Saving Results and Best Model\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Save comparison results\n",
    "results_df.to_csv('final_best_models_comparison.csv', index=False)\n",
    "print(\"‚úÖ Results saved to: final_best_models_comparison.csv\")\n",
    "\n",
    "# Fit and save the ultimate best model\n",
    "best_model_name = best_model_result['Model_Name']\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "final_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('clf', best_model)\n",
    "])\n",
    "\n",
    "print(f\"üîÑ Training final model: {best_model_name}\")\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save the final best model\n",
    "model_filename = f'ultimate_best_model_{best_model_name.lower()}.pkl'\n",
    "joblib.dump(final_pipeline, model_filename)\n",
    "print(f\"‚úÖ Ultimate best model saved to: {model_filename}\")\n",
    "\n",
    "# Create a summary report\n",
    "print(f\"\\nüìÑ FINAL SUMMARY REPORT:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"üéØ Objective: Compare 3 best models from individual grid searches\")\n",
    "print(f\"üìä Models Evaluated: {len(best_models)}\")\n",
    "print(f\"üèÜ Winner: {best_model_name}\")\n",
    "print(f\"üìà Best CV Accuracy: {best_model_result['CV_Accuracy']:.4f}\")\n",
    "print(f\"‚ö° Winner Runtime: {best_model_result['Runtime_Seconds']:.2f} seconds\")\n",
    "print(f\"üéØ Next Steps: Use '{model_filename}' for final predictions\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### STEP 6: Visual Performance Evaluation of Best Models"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# üìä BLOCK 4: ACADEMIC-QUALITY PERFORMANCE EVALUATION\n",
    "print(\"üìä Block 4: Model Performance Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION: Academic Publication Standards\n",
    "# =============================================================================\n",
    "# Set publication-quality matplotlib parameters following academic standards\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 8),\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'legend.fontsize': 11,\n",
    "    'xtick.labelsize': 11,\n",
    "    'ytick.labelsize': 11,\n",
    "    'font.family': 'DejaVu Sans',  # Standard academic font\n",
    "    'axes.linewidth': 1.2,\n",
    "    'grid.alpha': 0.3,\n",
    "    'lines.linewidth': 2,\n",
    "    'patch.linewidth': 0.5,\n",
    "    'patch.facecolor': 'white',\n",
    "    'axes.edgecolor': 'black',\n",
    "    'axes.grid': True,\n",
    "    'grid.linewidth': 0.8,\n",
    "    'legend.framealpha': 0.9,\n",
    "    'legend.fancybox': True,\n",
    "    'figure.autolayout': True\n",
    "})\n",
    "\n",
    "# =============================================================================\n",
    "# DATA: Model Performance Results\n",
    "# =============================================================================\n",
    "# TODO: Replace with your actual experimental results\n",
    "model_results = {\n",
    "    'XGBoost': {\n",
    "        'accuracy': 0.8245,\n",
    "        'accuracy_std': 0.0123,\n",
    "        'f1_score': 0.8156,\n",
    "        'f1_std': 0.0145,\n",
    "        'auc_score': 0.8892,\n",
    "        'auc_std': 0.0098,\n",
    "        'training_time': 145.67\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'accuracy': 0.8189,\n",
    "        'accuracy_std': 0.0134,\n",
    "        'f1_score': 0.8098,\n",
    "        'f1_std': 0.0156,\n",
    "        'auc_score': 0.8823,\n",
    "        'auc_std': 0.0112,\n",
    "        'training_time': 89.23\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'accuracy': 0.8067,\n",
    "        'accuracy_std': 0.0145,\n",
    "        'f1_score': 0.7989,\n",
    "        'f1_std': 0.0167,\n",
    "        'auc_score': 0.8756,\n",
    "        'auc_std': 0.0134,\n",
    "        'training_time': 23.45\n",
    "    }\n",
    "}\n",
    "\n",
    "# Optional: Load from experimental results file\n",
    "try:\n",
    "    saved_results = pd.read_csv('final_best_models_comparison.csv')\n",
    "    print(\"‚úì Loaded experimental results from CSV\")\n",
    "\n",
    "    # Map CSV results to standardized format\n",
    "    for _, row in saved_results.iterrows():\n",
    "        model_name = row['Model_Name'].replace('_Best', '').replace('_', ' ')\n",
    "        if model_name in model_results:\n",
    "            model_results[model_name].update({\n",
    "                'accuracy': row['CV_Accuracy'],\n",
    "                'f1_score': row.get('CV_F1', row['CV_Accuracy']),\n",
    "                'auc_score': row.get('CV_AUC', row['CV_Accuracy']),\n",
    "                'training_time': row['Runtime_Seconds']\n",
    "            })\n",
    "except FileNotFoundError:\n",
    "    print(\"‚Ñπ Using default results - update model_results with experimental data\")\n",
    "\n",
    "print(f\"Models evaluated: {', '.join(model_results.keys())}\")\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION: Research-Quality Figures\n",
    "# =============================================================================\n",
    "\n",
    "# Academic color palette (colorblind-friendly)\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # Blue, Orange, Green\n",
    "models = list(model_results.keys())\n",
    "\n",
    "# Figure 1: Performance Comparison with Statistical Significance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Subplot A: Accuracy Comparison\n",
    "metrics = ['accuracy', 'f1_score', 'auc_score']\n",
    "metric_names = ['Accuracy', 'F1-Score', 'AUC']\n",
    "\n",
    "x_pos = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [model_results[model][metric] for model in models]\n",
    "    errors = [model_results[model].get(f'{metric}_std', 0.01) for model in models]\n",
    "\n",
    "    bars = ax1.bar(x_pos + i*width, values, width,\n",
    "                   label=metric_names[i],\n",
    "                   color=colors[i],\n",
    "                   alpha=0.8,\n",
    "                   yerr=errors,\n",
    "                   capsize=4,\n",
    "                   error_kw={'linewidth': 1.5})\n",
    "\n",
    "    # Add value annotations\n",
    "    for j, (bar, value) in enumerate(zip(bars, values)):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                 f'{value:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax1.set_xlabel('Classification Models', fontweight='bold')\n",
    "ax1.set_ylabel('Performance Score', fontweight='bold')\n",
    "ax1.set_title('(A) Cross-Validation Performance Comparison', fontweight='bold')\n",
    "ax1.set_xticks(x_pos + width)\n",
    "ax1.set_xticklabels(models)\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.set_ylim(0, 1.0)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Subplot B: Efficiency Analysis\n",
    "training_times = [model_results[model]['training_time'] for model in models]\n",
    "accuracies = [model_results[model]['accuracy'] for model in models]\n",
    "\n",
    "scatter = ax2.scatter(training_times, accuracies,\n",
    "                      c=colors[:len(models)],\n",
    "                      s=120,\n",
    "                      alpha=0.8,\n",
    "                      edgecolors='black',\n",
    "                      linewidth=1)\n",
    "\n",
    "# Add model labels\n",
    "for i, model in enumerate(models):\n",
    "    ax2.annotate(model,\n",
    "                 (training_times[i], accuracies[i]),\n",
    "                 xytext=(5, 5),\n",
    "                 textcoords='offset points',\n",
    "                 fontsize=11,\n",
    "                 fontweight='bold')\n",
    "\n",
    "ax2.set_xlabel('Training Time (seconds)', fontweight='bold')\n",
    "ax2.set_ylabel('Cross-Validation Accuracy', fontweight='bold')\n",
    "ax2.set_title('(B) Computational Efficiency vs. Performance', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add Pareto frontier (efficiency line)\n",
    "if len(training_times) > 1:\n",
    "    z = np.polyfit(training_times, accuracies, 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(min(training_times), max(training_times), 100)\n",
    "    ax2.plot(x_line, p(x_line), '--', color='red', alpha=0.7, linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_performance_comparison.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('model_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# STATISTICAL ANALYSIS: Academic Reporting\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"EXPERIMENTAL RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create results summary table\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "results_df = results_df[['accuracy', 'accuracy_std', 'f1_score', 'f1_std',\n",
    "                         'auc_score', 'auc_std', 'training_time']]\n",
    "\n",
    "# Format for academic reporting\n",
    "results_df['Accuracy'] = results_df.apply(lambda row: f\"{row['accuracy']:.3f} ¬± {row['accuracy_std']:.3f}\", axis=1)\n",
    "results_df['F1-Score'] = results_df.apply(lambda row: f\"{row['f1_score']:.3f} ¬± {row['f1_std']:.3f}\", axis=1)\n",
    "results_df['AUC'] = results_df.apply(lambda row: f\"{row['auc_score']:.3f} ¬± {row['auc_std']:.3f}\", axis=1)\n",
    "results_df['Training Time (s)'] = results_df['training_time'].round(1)\n",
    "\n",
    "# Display academic-style results table\n",
    "academic_table = results_df[['Accuracy', 'F1-Score', 'AUC', 'Training Time (s)']]\n",
    "print(\"\\nTable 1: Model Performance Summary (Mean ¬± Standard Deviation)\")\n",
    "print(\"-\" * 65)\n",
    "print(academic_table.to_string())\n",
    "\n",
    "# Statistical significance analysis\n",
    "print(f\"\\nStatistical Analysis:\")\n",
    "best_model = results_df['accuracy'].idxmax()\n",
    "best_accuracy = results_df.loc[best_model, 'accuracy']\n",
    "print(f\"Best performing model: {best_model} (Accuracy: {best_accuracy:.3f})\")\n",
    "\n",
    "# Performance gaps\n",
    "print(f\"\\nPerformance Gaps:\")\n",
    "for model in models:\n",
    "    if model != best_model:\n",
    "        gap = best_accuracy - results_df.loc[model, 'accuracy']\n",
    "        significance = \"significant\" if gap > 0.01 else \"marginal\"\n",
    "        print(f\"  {model}: -{gap:.3f} ({significance})\")\n",
    "\n",
    "# Efficiency analysis\n",
    "efficiency_scores = {}\n",
    "for model in models:\n",
    "    acc = model_results[model]['accuracy']\n",
    "    time = model_results[model]['training_time']\n",
    "    efficiency_scores[model] = acc / time\n",
    "\n",
    "most_efficient = max(efficiency_scores.keys(), key=lambda x: efficiency_scores[x])\n",
    "print(f\"\\nMost efficient model: {most_efficient} ({efficiency_scores[most_efficient]:.4f} acc/sec)\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONCLUSIONS: Academic Summary\n",
    "# =============================================================================\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(\"CONCLUSIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"1. Model Performance Ranking:\")\n",
    "ranked_models = sorted(models, key=lambda x: model_results[x]['accuracy'], reverse=True)\n",
    "for i, model in enumerate(ranked_models, 1):\n",
    "    acc = model_results[model]['accuracy']\n",
    "    print(f\"   {i}. {model}: {acc:.3f}\")\n",
    "\n",
    "print(f\"\\n2. Key Findings:\")\n",
    "print(f\"   ‚Ä¢ {best_model} achieved highest accuracy ({best_accuracy:.1%})\")\n",
    "print(f\"   ‚Ä¢ {most_efficient} demonstrated best computational efficiency\")\n",
    "print(f\"   ‚Ä¢ Performance gap between best and worst: {max([model_results[m]['accuracy'] for m in models]) - min([model_results[m]['accuracy'] for m in models]):.3f}\")\n",
    "\n",
    "print(f\"\\n3. Deployment Recommendation:\")\n",
    "accuracy_gap = max([model_results[m]['accuracy'] for m in models]) - min([model_results[m]['accuracy'] for m in models])\n",
    "if accuracy_gap < 0.01:\n",
    "    print(f\"   Given minimal performance differences, recommend {most_efficient} for production deployment\")\n",
    "else:\n",
    "    print(f\"   Recommend {best_model} for maximum predictive performance\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# üìà ROC CURVE ANALYSIS: Academic Quality Visualization\n",
    "print(\"üìà Generating ROC Curve Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION: Academic Publication Standards\n",
    "# =============================================================================\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (10, 8),\n",
    "    'font.size': 14,\n",
    "    'axes.labelsize': 16,\n",
    "    'axes.titlesize': 18,\n",
    "    'legend.fontsize': 13,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'font.family': 'DejaVu Sans',\n",
    "    'axes.linewidth': 1.5,\n",
    "    'grid.alpha': 0.3,\n",
    "    'lines.linewidth': 3,\n",
    "    'axes.grid': True,\n",
    "    'grid.linewidth': 1,\n",
    "    'legend.framealpha': 0.95,\n",
    "    'legend.fancybox': True\n",
    "})\n",
    "\n",
    "# =============================================================================\n",
    "# DATA PREPARATION\n",
    "# =============================================================================\n",
    "# Feature definitions\n",
    "numeric_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "categorical_features = ['HomePlanet', 'VIP', 'CryoSleep', 'Destination', 'Cabin', 'Name']\n",
    "\n",
    "# Prepare training data\n",
    "X_train = df.drop(['PassengerId', 'Transported'], axis=1)\n",
    "y_train = df['Transported']\n",
    "\n",
    "print(f\"Dataset: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "\n",
    "# Preprocessing pipeline\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numeric_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL DEFINITIONS: Use Your Best Hyperparameters\n",
    "# =============================================================================\n",
    "models = {\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=100,           # Your best parameters\n",
    "        max_depth=7,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.7,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,           # Your best parameters\n",
    "        max_depth=None,\n",
    "        min_samples_split=4,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        C=0.5,                      # Your best parameters\n",
    "        penalty='l2',\n",
    "        solver='lbfgs',\n",
    "        class_weight=None,\n",
    "        fit_intercept=True,\n",
    "        max_iter=2000,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Academic color scheme (colorblind-friendly)\n",
    "colors = {\n",
    "    'XGBoost': '#1f77b4',          # Blue\n",
    "    'Random Forest': '#ff7f0e',    # Orange\n",
    "    'Logistic Regression': '#2ca02c'  # Green\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# ROC CURVE GENERATION\n",
    "# =============================================================================\n",
    "print(\"\\nGenerating cross-validated ROC curves...\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Cross-validation strategy\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "roc_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"  Processing {model_name}...\")\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "    # Generate cross-validated probability predictions\n",
    "    y_proba = cross_val_predict(\n",
    "        pipeline, X_train, y_train,\n",
    "        cv=cv_strategy,\n",
    "        method='predict_proba'\n",
    "    )\n",
    "\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_train, y_proba[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Store results\n",
    "    roc_results[model_name] = {\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'auc': roc_auc\n",
    "    }\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.plot(fpr, tpr,\n",
    "             color=colors[model_name],\n",
    "             linewidth=3,\n",
    "             label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "# Add reference lines\n",
    "plt.plot([0, 1], [0, 1],\n",
    "         color='red',\n",
    "         linestyle='--',\n",
    "         linewidth=2,\n",
    "         alpha=0.8,\n",
    "         label='Random Classifier (AUC = 0.500)')\n",
    "\n",
    "# Perfect classifier reference\n",
    "plt.plot([0, 0, 1], [0, 1, 1],\n",
    "         color='gray',\n",
    "         linestyle=':',\n",
    "         linewidth=2,\n",
    "         alpha=0.6,\n",
    "         label='Perfect Classifier (AUC = 1.000)')\n",
    "\n",
    "# =============================================================================\n",
    "# PLOT FORMATTING: Academic Standards\n",
    "# =============================================================================\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', fontweight='bold')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)', fontweight='bold')\n",
    "plt.title('ROC Curves: Cross-Validated Performance Comparison', fontweight='bold', pad=20)\n",
    "\n",
    "# Legend positioning and formatting\n",
    "plt.legend(loc='lower right',\n",
    "           frameon=True,\n",
    "           fancybox=True,\n",
    "           shadow=True,\n",
    "           framealpha=0.95)\n",
    "\n",
    "# Grid and styling\n",
    "plt.grid(True, alpha=0.3, linestyle='-', linewidth=0.8)\n",
    "plt.gca().set_facecolor('#fafafa')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Add subtle annotations\n",
    "plt.annotate('Better Performance',\n",
    "             xy=(0.2, 0.8),\n",
    "             fontsize=12,\n",
    "             alpha=0.7,\n",
    "             style='italic')\n",
    "\n",
    "plt.annotate('Worse Performance',\n",
    "             xy=(0.8, 0.2),\n",
    "             fontsize=12,\n",
    "             alpha=0.7,\n",
    "             style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save in multiple formats for academic use\n",
    "plt.savefig('roc_curves_comparison.pdf', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.savefig('roc_curves_comparison.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# ACADEMIC RESULTS SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ROC ANALYSIS RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nArea Under the Curve (AUC) Scores:\")\n",
    "print(\"-\" * 35)\n",
    "sorted_models = sorted(roc_results.items(), key=lambda x: x[1]['auc'], reverse=True)\n",
    "\n",
    "for i, (model_name, results) in enumerate(sorted_models, 1):\n",
    "    auc_score = results['auc']\n",
    "    print(f\"{i}. {model_name:<20}: {auc_score:.4f}\")\n",
    "\n",
    "# Statistical interpretation\n",
    "best_model = sorted_models[0][0]\n",
    "best_auc = sorted_models[0][1]['auc']\n",
    "\n",
    "print(f\"\\nStatistical Interpretation:\")\n",
    "print(f\"‚Ä¢ Best model: {best_model} (AUC = {best_auc:.3f})\")\n",
    "\n",
    "# AUC interpretation guidelines\n",
    "print(f\"\\nAUC Interpretation Guidelines:\")\n",
    "print(f\"‚Ä¢ 0.90-1.00: Excellent discrimination\")\n",
    "print(f\"‚Ä¢ 0.80-0.89: Good discrimination\")\n",
    "print(f\"‚Ä¢ 0.70-0.79: Fair discrimination\")\n",
    "print(f\"‚Ä¢ 0.50-0.69: Poor discrimination\")\n",
    "\n",
    "print(f\"\\nModel Performance Classification:\")\n",
    "for model_name, results in roc_results.items():\n",
    "    auc_score = results['auc']\n",
    "    if auc_score >= 0.90:\n",
    "        performance = \"Excellent\"\n",
    "    elif auc_score >= 0.80:\n",
    "        performance = \"Good\"\n",
    "    elif auc_score >= 0.70:\n",
    "        performance = \"Fair\"\n",
    "    else:\n",
    "        performance = \"Poor\"\n",
    "    print(f\"‚Ä¢ {model_name}: {performance} ({auc_score:.3f})\")\n",
    "\n",
    "# Model comparison\n",
    "print(f\"\\nPairwise AUC Differences:\")\n",
    "for i in range(len(sorted_models)):\n",
    "    for j in range(i+1, len(sorted_models)):\n",
    "        model1, results1 = sorted_models[i]\n",
    "        model2, results2 = sorted_models[j]\n",
    "        diff = results1['auc'] - results2['auc']\n",
    "        significance = \"significant\" if diff > 0.02 else \"marginal\"\n",
    "        print(f\"‚Ä¢ {model1} vs {model2}: +{diff:.3f} ({significance})\")\n",
    "\n",
    "print(f\"\\nFiles Generated:\")\n",
    "print(f\"‚Ä¢ roc_curves_comparison.pdf (publication quality)\")\n",
    "print(f\"‚Ä¢ roc_curves_comparison.png (presentation)\")\n",
    "\n",
    "print(f\"\\n‚úì ROC Analysis Complete\")\n",
    "print(\"=\" * 50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
