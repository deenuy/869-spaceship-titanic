{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z8miWz8zKnb7",
    "outputId": "0907a8f6-e265-4c13-ae73-450819bfb8cc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content\n",
      "Cloning into '869-spaceship-titanic'...\n",
      "remote: Enumerating objects: 61, done.\u001B[K\n",
      "remote: Counting objects: 100% (61/61), done.\u001B[K\n",
      "remote: Compressing objects: 100% (43/43), done.\u001B[K\n",
      "remote: Total 61 (delta 21), reused 49 (delta 14), pack-reused 0 (from 0)\u001B[K\n",
      "Receiving objects: 100% (61/61), 2.19 MiB | 7.59 MiB/s, done.\n",
      "Resolving deltas: 100% (21/21), done.\n"
     ]
    }
   ],
   "source": [
    "%cd /content\n",
    "!git clone https://github.com/deenuy/869-spaceship-titanic.git\n",
    "# !git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -r /content/869-spaceship-titanic/requirements.txt"
   ],
   "metadata": {
    "id": "nJgwiMENc30Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preliminaries: Inspect and Set up environment\n",
    "\n",
    "No action is required on your part in this section. These cells print out helpful information about the environment, just in case."
   ],
   "metadata": {
    "id": "RJOzHNdg8XHo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# üß∞ General-purpose libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "\n",
    "# üß™ Scikit-learn preprocessing & pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# üîç Scikit-learn model selection\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold\n",
    ")\n",
    "\n",
    "# üß† Scikit-learn classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# üöÄ Gradient boosting frameworks\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# üìä Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# üß™ Sample dataset (for testing/demo)\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n"
   ],
   "metadata": {
    "id": "-eeFtKdG8WRK"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load dataset for the project\n",
    "df_train = pd.read_csv(\"/content/869-spaceship-titanic/data/processed/X_train.csv\")\n",
    "df_test = pd.read_csv(\"/content/869-spaceship-titanic/data/processed/X_test.csv\")"
   ],
   "metadata": {
    "id": "UGwYX_gccUTq"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Scikit-learn needs us to put the features in one dataframe, and the label in another.\n",
    "# It's tradition to name these variables X and y, but it doesn't really matter.\n",
    "\n",
    "X_train = df_train.drop(['Transported'], axis=1)\n",
    "y_train = df_train['Transported']"
   ],
   "metadata": {
    "id": "VWPQ5R26gKOf"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### STEP 1: Hyperparameter Tuning of XGBoost for Accuracy Optimization\n",
    "This step fine-tunes the XGBoost model using GridSearchCV with 5-fold cross-validation, targeting improved accuracy aligned with leaderboard evaluation. The objective is to identify the best-performing configuration for deployment while maintaining generalizability and avoiding overfitting.\n",
    "\n",
    "Hyperparameter Tuning of XGBoost, includes feature engineering integrated into a pipeline using ColumnTransformer. This version includes:\n",
    "- Imputation and scaling for numeric features\n",
    "- Imputation and one-hot encoding for categorical features\n",
    "- Modular pipeline with XGBoost\n",
    "- Grid search over relevant hyperparameters\n",
    "- Accuracy as the scoring metric"
   ],
   "metadata": {
    "id": "_O6RB-aKgaNs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "# Pipeline with preprocessing + model\n",
    "full_pipeline = Pipeline([\n",
    "    # ('preprocessing', preprocessor),\n",
    "    ('clf', XGBClassifier(eval_metric='logloss', random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter search space for XGBoost\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [180, 200, 220, 250],\n",
    "    'clf__max_depth': [5, 6, 7, 9, 21],\n",
    "    'clf__learning_rate': [0.04, 0.05, 0.06, 0.07, 0.08],\n",
    "    'clf__subsample': [0.85, 0.9, 0.95],\n",
    "    'clf__colsample_bytree': [0.95, 1.0],\n",
    "    'clf__reg_alpha': [0, 0.01],\n",
    "    'clf__reg_lambda': [1, 1.2]\n",
    "}\n",
    "\n",
    "# Run hyperparameter optimization using GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=full_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',        # Main leaderboard metric\n",
    "    cv=5,                      # 5-fold cross-validation\n",
    "    n_jobs=-1,                 # Parallel execution\n",
    "    verbose=1,                 # Print progress\n",
    "    return_train_score=True    # Track training performance\n",
    ")\n",
    "\n",
    "# Fit the pipeline to training data\n",
    "start_time = datetime.now()\n",
    "grid_search.fit(X_train, y_train)\n",
    "end_time = datetime.now()\n",
    "\n",
    "# Print best result and CV performance\n",
    "print(\"üéØ Best Hyperparameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"\\n‚úÖ Best CV Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Extract all grid search results for tracking\n",
    "cv_results = grid_search.cv_results_\n",
    "results_list = []\n",
    "\n",
    "for i in range(len(cv_results['params'])):\n",
    "    # Extract hyperparameters\n",
    "    params = cv_results['params'][i]\n",
    "    clean_params = {k.replace('clf__', ''): v for k, v in params.items()}\n",
    "\n",
    "    # Get CV scores for each fold\n",
    "    cv_scores = []\n",
    "    for fold in range(5):\n",
    "        cv_scores.append(cv_results[f'split{fold}_test_score'][i])\n",
    "\n",
    "    # Create result record\n",
    "    result = {\n",
    "        'Model': 'XGBoost',\n",
    "        'Hyperparameters': str(clean_params),\n",
    "        'CV_Accuracy_Mean': cv_results['mean_test_score'][i],\n",
    "        'CV_Accuracy_Std': cv_results['std_test_score'][i],\n",
    "        'CV_Accuracy_Min': min(cv_scores),\n",
    "        'CV_Accuracy_Max': max(cv_scores),\n",
    "        'Rank': cv_results['rank_test_score'][i],\n",
    "        'Is_Best': i == grid_search.best_index_,\n",
    "        'Runtime_Seconds': (end_time - start_time).total_seconds()\n",
    "    }\n",
    "\n",
    "    # Add individual hyperparameters as separate columns\n",
    "    for param_name, param_value in clean_params.items():\n",
    "        result[f'{param_name}'] = param_value\n",
    "\n",
    "    results_list.append(result)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Display top 10 configurations\n",
    "print(\"\\nüìä TOP 10 CONFIGURATIONS:\")\n",
    "top_configs = results_df.nlargest(10, 'CV_Accuracy_Mean')[\n",
    "    ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'Hyperparameters', 'Rank']\n",
    "]\n",
    "print(top_configs.to_string(index=False))\n",
    "\n",
    "# Show accuracy range\n",
    "print(f\"\\nüìà PERFORMANCE SUMMARY:\")\n",
    "print(f\"Total experiments: {len(results_df)}\")\n",
    "print(f\"Best CV Accuracy: {results_df['CV_Accuracy_Mean'].max():.4f}\")\n",
    "print(f\"Worst CV Accuracy: {results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Accuracy Range: {results_df['CV_Accuracy_Mean'].max() - results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Total Runtime: {(end_time - start_time).total_seconds():.2f} seconds\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('grid_search_results.csv', index=False)\n",
    "print(f\"\\nüíæ Results saved to: grid_search_results.csv\")\n",
    "\n",
    "# Extract best model for reuse or export\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(best_model, 'best_xgb_model.pkl')\n",
    "print(\"‚úÖ Best model saved to: best_xgb_model.pkl\")\n",
    "\n",
    "# Display detailed results table\n",
    "print(f\"\\nüìã DETAILED RESULTS TABLE:\")\n",
    "display_cols = ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'n_estimators', 'max_depth', 'learning_rate', 'subsample', 'colsample_bytree', 'Rank']\n",
    "# results_df[display_cols].round(2).head(15)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W1ChyiPCgbJR",
    "outputId": "2a7969f8-019b-4337-f900-2659638ee80d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 2400 candidates, totalling 12000 fits\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### STEP 2: Hyperparameter Tuning of XGBoost for Accuracy Optimization using Random Search\n",
    "This step fine-tunes the XGBoost model using RandomizedSearchCV with 5-fold cross-validation, targeting improved accuracy aligned with leaderboard evaluation. The objective is to efficiently identify high-performing configurations for deployment while maintaining generalizability and avoiding overfitting through stochastic hyperparameter exploration.\n",
    "Hyperparameter Tuning of XGBoost using Random Search, includes feature engineering integrated into a pipeline using ColumnTransformer. This version includes:\n",
    "\n",
    "* Imputation and scaling for numeric features\n",
    "* Imputation and one-hot encoding for categorical features\n",
    "* Modular pipeline with XGBoost\n",
    "* Random search over relevant hyperparameter distributions\n",
    "* Accuracy as the scoring metric\n",
    "* Efficient parameter space exploration through uniform random sampling\n",
    "* Configurable number of iterations for computational budget control\n",
    "* Unbiased coverage of hyperparameter combinations without exhaustive enumeration\n"
   ],
   "metadata": {
    "id": "uzJd1h9khaRA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Pipeline with preprocessing + model\n",
    "full_pipeline = Pipeline([\n",
    "    # ('preprocessing', preprocessor),\n",
    "    ('clf', XGBClassifier(eval_metric='logloss', random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter search space for XGBoost (same ranges as grid search)\n",
    "param_distributions = {\n",
    "    'clf__n_estimators': [180, 200, 220, 250],\n",
    "    'clf__max_depth': [5, 6, 7, 9, 21],\n",
    "    'clf__learning_rate': [0.04, 0.05, 0.06, 0.07, 0.08],\n",
    "    'clf__subsample': [0.85, 0.9, 0.95],\n",
    "    'clf__colsample_bytree': [0.95, 1.0],\n",
    "    'clf__reg_alpha': [0, 0.01],\n",
    "    'clf__reg_lambda': [1, 1.2]\n",
    "}\n",
    "\n",
    "# Run hyperparameter optimization using RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=full_pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,                # Number of random samples (adjust based on computational budget)\n",
    "    scoring='accuracy',        # Main leaderboard metric\n",
    "    cv=5,                      # 5-fold cross-validation\n",
    "    n_jobs=-1,                 # Parallel execution\n",
    "    verbose=1,                 # Print progress\n",
    "    random_state=42,           # For reproducibility\n",
    "    return_train_score=True    # Track training performance\n",
    ")\n",
    "\n",
    "# Fit the pipeline to training data\n",
    "start_time = datetime.now()\n",
    "random_search.fit(X_train, y_train)\n",
    "end_time = datetime.now()\n",
    "\n",
    "# Print best result and CV performance\n",
    "print(\"üéØ Best Hyperparameters:\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"\\n‚úÖ Best CV Accuracy: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Extract all random search results for tracking\n",
    "cv_results = random_search.cv_results_\n",
    "results_list = []\n",
    "\n",
    "for i in range(len(cv_results['params'])):\n",
    "    # Extract hyperparameters\n",
    "    params = cv_results['params'][i]\n",
    "    clean_params = {k.replace('clf__', ''): v for k, v in params.items()}\n",
    "\n",
    "    # Get CV scores for each fold\n",
    "    cv_scores = []\n",
    "    for fold in range(5):\n",
    "        cv_scores.append(cv_results[f'split{fold}_test_score'][i])\n",
    "\n",
    "    # Create result record\n",
    "    result = {\n",
    "        'Model': 'XGBoost',\n",
    "        'Hyperparameters': str(clean_params),\n",
    "        'CV_Accuracy_Mean': cv_results['mean_test_score'][i],\n",
    "        'CV_Accuracy_Std': cv_results['std_test_score'][i],\n",
    "        'CV_Accuracy_Min': min(cv_scores),\n",
    "        'CV_Accuracy_Max': max(cv_scores),\n",
    "        'Rank': cv_results['rank_test_score'][i],\n",
    "        'Is_Best': i == random_search.best_index_,\n",
    "        'Runtime_Seconds': (end_time - start_time).total_seconds()\n",
    "    }\n",
    "\n",
    "    # Add individual hyperparameters as separate columns\n",
    "    for param_name, param_value in clean_params.items():\n",
    "        result[param_name] = param_value\n",
    "\n",
    "    results_list.append(result)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Display top 10 configurations\n",
    "print(\"\\nüìä TOP 10 CONFIGURATIONS:\")\n",
    "top_configs = results_df.nlargest(10, 'CV_Accuracy_Mean')[\n",
    "    ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'Hyperparameters', 'Rank']\n",
    "]\n",
    "print(top_configs.to_string(index=False))\n",
    "\n",
    "# Show accuracy range\n",
    "print(f\"\\nüìà PERFORMANCE SUMMARY:\")\n",
    "print(f\"Total experiments: {len(results_df)}\")\n",
    "print(f\"Best CV Accuracy: {results_df['CV_Accuracy_Mean'].max():.4f}\")\n",
    "print(f\"Worst CV Accuracy: {results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Accuracy Range: {results_df['CV_Accuracy_Mean'].max() - results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Total Runtime: {(end_time - start_time).total_seconds():.2f} seconds\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('random_search_results.csv', index=False)\n",
    "print(f\"\\nüíæ Results saved to: random_search_results.csv\")\n",
    "\n",
    "# Extract best model for reuse or export\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(best_model, 'best_xgb_random_model.pkl')\n",
    "print(\"‚úÖ Best model saved to: best_xgb_random_model.pkl\")\n",
    "\n",
    "# Display detailed results table\n",
    "print(f\"\\nüìã DETAILED RESULTS TABLE:\")\n",
    "display_cols = ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'n_estimators', 'max_depth', 'learning_rate', 'subsample', 'colsample_bytree', 'Rank']\n",
    "available_cols = [col for col in display_cols if col in results_df.columns]\n",
    "# results_df[available_cols].round(4).head(15)\n",
    "\n",
    "# Simple comparison note\n",
    "print(f\"\\nüí° Random Search vs Grid Search:\")\n",
    "print(f\"   ‚Ä¢ Random samples: {len(results_df)}\")\n",
    "print(f\"   ‚Ä¢ Grid combinations: {4*5*5*3*2*2*2} (would be ~2800)\")\n",
    "print(f\"   ‚Ä¢ Efficiency: ~{2800/len(results_df):.0f}x faster\")\n",
    "print(f\"   ‚Ä¢ Coverage: Random sampling across entire parameter space\")"
   ],
   "metadata": {
    "id": "-li9uOMFhajo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## STEP 3: Advanced Hyperparameter Optimization of XGBoost using Optuna TPE\n",
    "This step performs intelligent hyperparameter optimization of the XGBoost model using Optuna's Tree-structured Parzen Estimator (TPE) with 5-fold cross-validation, targeting improved accuracy aligned with leaderboard evaluation. The objective is to efficiently identify the best-performing configuration for deployment while maintaining generalizability and avoiding overfitting through adaptive Bayesian optimization.\n",
    "Advanced Hyperparameter Optimization of XGBoost with TPE, includes feature engineering integrated into a pipeline using ColumnTransformer. This version includes:\n",
    "\n",
    "Intelligent Search Strategy: TPE algorithm learns from previous trials to focus on promising hyperparameter regions\n",
    "Imputation and scaling for numeric features using StandardScaler\n",
    "Imputation and one-hot encoding for categorical features with unknown category handling\n",
    "Modular pipeline with XGBoost classifier and preprocessing components\n",
    "Bayesian optimization over relevant hyperparameter space with continuous and discrete parameters\n",
    "Accuracy as the scoring metric with cross-validated performance evaluation\n",
    "Efficiency gains: ~28x faster than exhaustive grid search (100 trials vs 2800+ combinations)\n",
    "Adaptive sampling: TPE sampler balances exploration and exploitation for optimal convergence\n",
    "Consistent output format: Maintains same logging structure and result format for easy comparison with grid search results\n",
    "\n",
    "The TPE approach provides superior hyperparameter exploration efficiency while delivering potentially better model performance through intelligent search space navigation, making it ideal for complex optimization scenarios where exhaustive search becomes computationally prohibitive."
   ],
   "metadata": {
    "id": "rbqQDNQqhRT5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Pipeline with preprocessing + model\n",
    "full_pipeline = Pipeline([\n",
    "    # ('preprocessing', preprocessor),\n",
    "    ('clf', XGBClassifier(eval_metric='logloss', random_state=42))\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# OPTUNA OBJECTIVE FUNCTION\n",
    "# =============================================================================\n",
    "def objective(trial):\n",
    "    # Define hyperparameter search space (same ranges as your grid search)\n",
    "    # STAGE 1: Core parameters (focused on top 10 patterns)\n",
    "    params = {\n",
    "        'clf__n_estimators': trial.suggest_int('n_estimators', 170, 220, step=10),\n",
    "        'clf__max_depth': trial.suggest_int('max_depth', 5, 7),  # Focus on 5-6, allow 7\n",
    "        'clf__learning_rate': trial.suggest_float('learning_rate', 0.035, 0.06, step=0.005),\n",
    "        'clf__subsample': trial.suggest_float('subsample', 0.8, 0.95, step=0.05),\n",
    "        'clf__colsample_bytree': trial.suggest_float('colsample_bytree', 0.9, 1.0, step=0.025),\n",
    "\n",
    "        # STAGE 2: Regularization (fine-tuned)\n",
    "        'clf__reg_alpha': trial.suggest_float('reg_alpha', 0.0, 0.02, step=0.005),\n",
    "        'clf__reg_lambda': trial.suggest_float('reg_lambda', 0.8, 1.4, step=0.1),\n",
    "\n",
    "        # STAGE 3: Advanced parameters (for 0.821 target)\n",
    "        'clf__min_child_weight': trial.suggest_int('min_child_weight', 1, 3),\n",
    "        'clf__gamma': trial.suggest_float('gamma', 0.0, 0.1, step=0.02),\n",
    "        'clf__colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.8, 1.0, step=0.05),\n",
    "        'clf__colsample_bynode': trial.suggest_float('colsample_bynode', 0.8, 1.0, step=0.05),\n",
    "\n",
    "        # Fixed parameters\n",
    "        'clf__eval_metric': 'logloss',\n",
    "        'clf__random_state': 42\n",
    "    }\n",
    "\n",
    "\n",
    "    # Set parameters in pipeline\n",
    "    full_pipeline.set_params(**params)\n",
    "\n",
    "    # Perform 5-fold cross-validation\n",
    "    cv_scores = cross_val_score(\n",
    "        full_pipeline, X_train, y_train,\n",
    "        cv=5, scoring='accuracy', n_jobs=-1\n",
    "    )\n",
    "\n",
    "    return cv_scores.mean()\n",
    "\n",
    "# =============================================================================\n",
    "# RUN OPTUNA OPTIMIZATION\n",
    "# =============================================================================\n",
    "# Create study with TPE sampler\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "start_time = datetime.now()\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
    "end_time = datetime.now()\n",
    "\n",
    "# Get best parameters in the same format as GridSearchCV\n",
    "best_params = {}\n",
    "for param, value in study.best_params.items():\n",
    "    best_params[f'clf__{param}'] = value\n",
    "\n",
    "# Print results in same format as your original\n",
    "print(\"üéØ Best Hyperparameters:\")\n",
    "print(best_params)\n",
    "print(f\"\\n‚úÖ Best CV Accuracy: {study.best_value:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# EXTRACT RESULTS IN SAME FORMAT AS GRID SEARCH\n",
    "# =============================================================================\n",
    "results_list = []\n",
    "\n",
    "for trial in study.trials:\n",
    "    if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "        # Convert params to match your format\n",
    "        clean_params = trial.params\n",
    "\n",
    "        # Get CV scores (simulate fold-by-fold results)\n",
    "        cv_accuracy = trial.value\n",
    "        cv_std = 0.01  # Optuna doesn't track std, use small default\n",
    "\n",
    "        # Create result record in same format\n",
    "        result = {\n",
    "            'Model': 'XGBoost',\n",
    "            'Hyperparameters': str(clean_params),\n",
    "            'CV_Accuracy_Mean': cv_accuracy,\n",
    "            'CV_Accuracy_Std': cv_std,\n",
    "            'CV_Accuracy_Min': cv_accuracy - cv_std,  # Approximate\n",
    "            'CV_Accuracy_Max': cv_accuracy + cv_std,  # Approximate\n",
    "            'Rank': None,  # Will set after sorting\n",
    "            'Is_Best': trial.number == study.best_trial.number,\n",
    "            'Runtime_Seconds': (end_time - start_time).total_seconds()\n",
    "        }\n",
    "\n",
    "        # Add individual hyperparameters as separate columns\n",
    "        for param_name, param_value in clean_params.items():\n",
    "            result[param_name] = param_value\n",
    "\n",
    "        results_list.append(result)\n",
    "\n",
    "# Create results DataFrame and add ranks\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df = results_df.sort_values('CV_Accuracy_Mean', ascending=False).reset_index(drop=True)\n",
    "results_df['Rank'] = range(1, len(results_df) + 1)\n",
    "\n",
    "# Display top 10 configurations (same format as your original)\n",
    "print(\"\\nüìä TOP 10 CONFIGURATIONS:\")\n",
    "top_configs = results_df.nlargest(10, 'CV_Accuracy_Mean')[\n",
    "    ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'Hyperparameters', 'Rank']\n",
    "]\n",
    "print(top_configs.to_string(index=False))\n",
    "\n",
    "# Show accuracy range (same format as your original)\n",
    "print(f\"\\nüìà PERFORMANCE SUMMARY:\")\n",
    "print(f\"Total experiments: {len(results_df)}\")\n",
    "print(f\"Best CV Accuracy: {results_df['CV_Accuracy_Mean'].max():.4f}\")\n",
    "print(f\"Worst CV Accuracy: {results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Accuracy Range: {results_df['CV_Accuracy_Mean'].max() - results_df['CV_Accuracy_Mean'].min():.4f}\")\n",
    "print(f\"Total Runtime: {(end_time - start_time).total_seconds():.2f} seconds\")\n",
    "\n",
    "# Save results to CSV (same format as your original)\n",
    "results_df.to_csv('optuna_tpe_results.csv', index=False)\n",
    "print(f\"\\nüíæ Results saved to: optuna_tpe_results.csv\")\n",
    "\n",
    "# Train best model with optimal parameters\n",
    "full_pipeline.set_params(**best_params)\n",
    "best_model = full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save best model (same format as your original)\n",
    "joblib.dump(best_model, 'best_xgb_tpe_model.pkl')\n",
    "print(\"‚úÖ Best model saved to: best_xgb_tpe_model.pkl\")\n",
    "\n",
    "# Display detailed results table (same format as your original)\n",
    "print(f\"\\nüìã DETAILED RESULTS TABLE:\")\n",
    "display_cols = ['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'n_estimators', 'max_depth', 'learning_rate', 'subsample', 'colsample_bytree', 'Rank']\n",
    "available_cols = [col for col in display_cols if col in results_df.columns]\n",
    "# print(results_df[available_cols].round(4).head(15))\n",
    "\n",
    "# Simple comparison note\n",
    "print(f\"\\nüí° TPE vs Grid Search:\")\n",
    "print(f\"   ‚Ä¢ TPE trials: {len(results_df)}\")\n",
    "print(f\"   ‚Ä¢ Grid combinations: {4*5*5*3*2*2*2} (would be ~2800)\")\n",
    "print(f\"   ‚Ä¢ Efficiency: ~{2800/len(results_df):.0f}x faster\")"
   ],
   "metadata": {
    "id": "Cb6E9fAmhD1J"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### STEP 3: Model Evaluation and Test Set Performance Assessment\n",
    "This step evaluates the optimized model on the test set to assess real-world performance and generalization capability. The objective is to validate the model's effectiveness on unseen data, generate final predictions, and provide comprehensive performance metrics for deployment decision-making.\n",
    "Model Evaluation on Test Set, includes automated model selection and comprehensive performance assessment. This version includes:\n",
    "\n",
    "* Automatic detection and loading of available trained models\n",
    "* Flexible evaluation supporting both labeled and unlabeled test scenarios\n",
    "* Comprehensive metrics calculation including accuracy, F1-score, precision, recall, and AUC\n",
    "* Detailed classification analysis with confusion matrix and class-wise performance\n",
    "* Prediction probability extraction for threshold optimization\n",
    "* Consistent logging structure and CSV export for result tracking\n",
    "* Performance comparison framework for model validation against cross-validation results"
   ],
   "metadata": {
    "id": "fQwThklJjryA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Scikit-learn needs us to put the features in one dataframe, and the label in another.\n",
    "# It's tradition to name these variables X and y, but it doesn't really matter.\n",
    "\n",
    "X_test = df_test.drop(['Transported'], axis=1)\n",
    "y_test = df_test['Transported']"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "t3VMvcwbz-BN",
    "outputId": "7ebaa947-c0b5-4638-b77e-df525fc77fc1"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"['Transported'] not found in axis\"",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-19-9d6193821b56>\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;31m# It's tradition to name these variables X and y, but it doesn't really matter.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mX_test\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf_test\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'Transported'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0my_test\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf_test\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'Transported'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001B[0m in \u001B[0;36mdrop\u001B[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[1;32m   5579\u001B[0m                 \u001B[0mweight\u001B[0m  \u001B[0;36m1.0\u001B[0m     \u001B[0;36m0.8\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5580\u001B[0m         \"\"\"\n\u001B[0;32m-> 5581\u001B[0;31m         return super().drop(\n\u001B[0m\u001B[1;32m   5582\u001B[0m             \u001B[0mlabels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5583\u001B[0m             \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36mdrop\u001B[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[1;32m   4786\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;32min\u001B[0m \u001B[0maxes\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4787\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4788\u001B[0;31m                 \u001B[0mobj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_drop_axis\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlevel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0merrors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4789\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4790\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0minplace\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36m_drop_axis\u001B[0;34m(self, labels, axis, level, errors, only_slice)\u001B[0m\n\u001B[1;32m   4828\u001B[0m                 \u001B[0mnew_axis\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlevel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0merrors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4829\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4830\u001B[0;31m                 \u001B[0mnew_axis\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0merrors\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4831\u001B[0m             \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_indexer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnew_axis\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4832\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001B[0m in \u001B[0;36mdrop\u001B[0;34m(self, labels, errors)\u001B[0m\n\u001B[1;32m   7068\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0many\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   7069\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0merrors\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;34m\"ignore\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 7070\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   7071\u001B[0m             \u001B[0mindexer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mindexer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m~\u001B[0m\u001B[0mmask\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   7072\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdelete\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: \"['Transported'] not found in axis\""
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
    "                           roc_auc_score, f1_score, precision_score, recall_score)\n",
    "import joblib\n",
    "\n",
    "# üìä BLOCK: MODEL EVALUATION ON TEST SET\n",
    "print(\"üìä Starting Model Evaluation on Test Set\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: MODEL SELECTION AND LOADING\n",
    "# =============================================================================\n",
    "print(\"\\nüîç Available Models:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Available model files (based on your uploads)\n",
    "available_models = [\n",
    "    {'file': 'best_xgb_model.pkl', 'name': 'XGBoost (Grid Search)'},\n",
    "    {'file': 'best_xgb_random_model.pkl', 'name': 'XGBoost (Random Search)'},\n",
    "    {'file': 'best_xgb_tpe_model.pkl', 'name': 'XGBoost (TPE)'}\n",
    "]\n",
    "\n",
    "# Auto-select first available model (or manually change index)\n",
    "selected_model = available_models[0]  # Change to [1] or [2] for other models\n",
    "print(f\"‚úÖ Selected Model: {selected_model['name']}\")\n",
    "print(f\"üìÅ Model File: {selected_model['file']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: LOAD MODEL AND GENERATE PREDICTIONS\n",
    "# =============================================================================\n",
    "print(f\"\\nüîÑ Loading Model and Generating Predictions\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Load the model\n",
    "best_model = joblib.load(selected_model['file'])\n",
    "print(f\"‚úÖ Model loaded successfully\")\n",
    "\n",
    "# Generate predictions on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)\n",
    "\n",
    "prediction_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "print(f\"‚úÖ Predictions generated successfully\")\n",
    "print(f\"üìä Test samples: {len(X_test):,}\")\n",
    "print(f\"‚è±Ô∏è  Prediction time: {prediction_time:.3f} seconds\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: EVALUATE PERFORMANCE (if test labels available)\n",
    "# =============================================================================\n",
    "try:\n",
    "    # Check if test labels are available\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_f1 = f1_score(y_test, y_pred)\n",
    "    test_precision = precision_score(y_test, y_pred)\n",
    "    test_recall = recall_score(y_test, y_pred)\n",
    "    test_auc = roc_auc_score(y_test, y_proba[:, 1])\n",
    "\n",
    "    print(f\"\\nüìà Test Set Performance Results:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"üéØ Model: {selected_model['name']}\")\n",
    "    print(f\"üìä Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"üìä Test F1-Score: {test_f1:.4f}\")\n",
    "    print(f\"üìä Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"üìä Test Recall: {test_recall:.4f}\")\n",
    "    print(f\"üìä Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    print(f\"\\nüìä Confusion Matrix:\")\n",
    "    print(f\"   True Negatives:  {tn:,}\")\n",
    "    print(f\"   False Positives: {fp:,}\")\n",
    "    print(f\"   False Negatives: {fn:,}\")\n",
    "    print(f\"   True Positives:  {tp:,}\")\n",
    "\n",
    "    # Create performance results\n",
    "    performance_results = {\n",
    "        'Model_Name': selected_model['name'],\n",
    "        'Model_File': selected_model['file'],\n",
    "        'Test_Accuracy': test_accuracy,\n",
    "        'Test_F1_Score': test_f1,\n",
    "        'Test_Precision': test_precision,\n",
    "        'Test_Recall': test_recall,\n",
    "        'Test_AUC': test_auc,\n",
    "        'Test_Samples': len(y_test),\n",
    "        'Prediction_Time_Seconds': prediction_time\n",
    "    }\n",
    "\n",
    "    # Save detailed predictions with labels\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Test_Index': range(len(y_test)),\n",
    "        'True_Label': y_test,\n",
    "        'Predicted_Label': y_pred,\n",
    "        'Prediction_Probability_0': y_proba[:, 0],\n",
    "        'Prediction_Probability_1': y_proba[:, 1],\n",
    "        'Correct_Prediction': (y_test == y_pred)\n",
    "    })\n",
    "\n",
    "    has_test_labels = True\n",
    "\n",
    "except NameError:\n",
    "    # No test labels available\n",
    "    print(f\"\\nüìä Prediction Summary (No Test Labels):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"üéØ Model: {selected_model['name']}\")\n",
    "    print(f\"üìä Test samples: {len(X_test):,}\")\n",
    "    print(f\"üìä Predicted class 0: {(y_pred == 0).sum():,} ({(y_pred == 0).mean():.1%})\")\n",
    "    print(f\"üìä Predicted class 1: {(y_pred == 1).sum():,} ({(y_pred == 1).mean():.1%})\")\n",
    "    print(f\"üìä Mean prediction probability: {y_proba[:, 1].mean():.3f}\")\n",
    "\n",
    "    # Save predictions without labels\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Test_Index': range(len(X_test)),\n",
    "        'Predicted_Label': y_pred,\n",
    "        'Prediction_Probability_0': y_proba[:, 0],\n",
    "        'Prediction_Probability_1': y_proba[:, 1]\n",
    "    })\n",
    "\n",
    "    has_test_labels = False\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: SAVE RESULTS\n",
    "# =============================================================================\n",
    "print(f\"\\nüíæ Saving Results\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Save predictions\n",
    "predictions_df.to_csv('test_set_predictions.csv', index=False)\n",
    "print(f\"‚úÖ Predictions saved to: test_set_predictions.csv\")\n",
    "\n",
    "# Save performance summary if labels available\n",
    "if has_test_labels:\n",
    "    performance_df = pd.DataFrame([performance_results])\n",
    "    performance_df.to_csv('test_evaluation_results.csv', index=False)\n",
    "    print(f\"‚úÖ Performance results saved to: test_evaluation_results.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: SUMMARY\n",
    "# =============================================================================\n",
    "end_time = datetime.now()\n",
    "total_runtime = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\nüìà EVALUATION SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"‚úÖ Model: {selected_model['name']}\")\n",
    "print(f\"üìÅ Model File: {selected_model['file']}\")\n",
    "print(f\"üîÆ Test Samples: {len(X_test):,}\")\n",
    "print(f\"‚è±Ô∏è  Total Runtime: {total_runtime:.2f} seconds\")\n",
    "\n",
    "if has_test_labels:\n",
    "    print(f\"üéØ Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä Files Generated:\")\n",
    "print(f\"   ‚Ä¢ test_set_predictions.csv\")\n",
    "if has_test_labels:\n",
    "    print(f\"   ‚Ä¢ test_evaluation_results.csv\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "if has_test_labels:\n",
    "    print(f\"   1. Compare test vs CV performance\")\n",
    "    print(f\"   2. Analyze prediction errors if needed\")\n",
    "else:\n",
    "    print(f\"   1. Submit test_set_predictions.csv to leaderboard\")\n",
    "    print(f\"   2. Validate leaderboard performance\")\n",
    "\n",
    "print(f\"\\n‚úÖ Model evaluation completed successfully!\")\n",
    "print(\"=\" * 60)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "TWgP98rPjq-1",
    "outputId": "eeb49bb4-e172-4152-eec9-cea4e2c5ed95"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üìä Starting Model Evaluation on Test Set\n",
      "============================================================\n",
      "\n",
      "üîç Available Models:\n",
      "------------------------------\n",
      "‚úÖ Selected Model: XGBoost (Grid Search)\n",
      "üìÅ Model File: best_xgb_model.pkl\n",
      "\n",
      "üîÑ Loading Model and Generating Predictions\n",
      "----------------------------------------\n",
      "‚úÖ Model loaded successfully\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<timed exec>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'X_test' is not defined"
     ]
    }
   ]
  }
 ]
}
