{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 100411,
     "databundleVersionId": 12064814,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  },
  "colab": {
   "name": "Template Notebook",
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat_minor": 0,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/stepthom/869_course/blob/main/2026%20869%20Project%20Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MMAI 869 Project: Example Notebook\n",
    "\n",
    "*Updated May 1, 2025*\n",
    "\n",
    "This notebook serves as a template for the Team Project. Teams can use this notebook as a starting point, and update it successively with new ideas and techniques to improve their model results.\n",
    "\n",
    "Note that is not required to use this template. Teams may also alter this template in any way they see fit."
   ],
   "metadata": {
    "id": "T_JqF4nhnHAK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preliminaries: Inspect and Set up environment\n",
    "\n",
    "No action is required on your part in this section. These cells print out helpful information about the environment, just in case."
   ],
   "metadata": {
    "id": "5h8kN7e4nHAM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# ğŸ§° General-purpose libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "\n",
    "# ğŸ§ª Scikit-learn preprocessing & pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# ğŸ” Scikit-learn model selection\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold\n",
    ")\n",
    "\n",
    "# ğŸ§  Scikit-learn classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# ğŸš€ Gradient boosting frameworks\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# ğŸ“Š Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ğŸ§ª Sample dataset (for testing/demo)\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-29T15:33:14.024124Z",
     "iopub.execute_input": "2025-04-29T15:33:14.024385Z",
     "iopub.status.idle": "2025-04-29T15:33:23.963717Z",
     "shell.execute_reply.started": "2025-04-29T15:33:14.024359Z",
     "shell.execute_reply": "2025-04-29T15:33:23.962752Z"
    },
    "id": "iFGZKq7dnHAN",
    "ExecuteTime": {
     "end_time": "2025-06-06T19:03:00.667785Z",
     "start_time": "2025-06-06T19:03:00.238751Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "!python --version"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-29T15:34:57.648204Z",
     "iopub.execute_input": "2025-04-29T15:34:57.64854Z",
     "iopub.status.idle": "2025-04-29T15:34:57.787301Z",
     "shell.execute_reply.started": "2025-04-29T15:34:57.648514Z",
     "shell.execute_reply": "2025-04-29T15:34:57.786221Z"
    },
    "id": "M8P3CPDmnHAP",
    "ExecuteTime": {
     "end_time": "2025-06-06T19:03:03.994287Z",
     "start_time": "2025-06-06T19:03:03.853643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.16\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0: Data Loading and Inspection"
   ],
   "metadata": {
    "id": "c5EsSXF-nHAQ"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T19:03:05.241606Z",
     "start_time": "2025-06-06T19:03:05.212822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ================================================================\n",
    "# LOAD TRAINING DATA WITH SELECTED FEATURES\n",
    "# ================================================================\n",
    "\n",
    "print(\"ğŸ“¥ LOADING TRAINING DATA WITH SELECTED FEATURES\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Load complete processed training dataset\n",
    "df_processed = pd.read_csv('../data/processed/train_features_engineered.csv')\n",
    "print(f\"   âœ… Training dataset loaded: {df_processed.shape}\")\n",
    "\n",
    "# Load selected features from feature selection phase\n",
    "best_features = pd.read_csv('../data/processed/best_features_selected.csv')['best_features'].tolist()\n",
    "print(f\"   âœ… Selected features loaded: {len(best_features)} features\")\n",
    "\n",
    "# Filter training data using selected features only\n",
    "X_train = df_processed[best_features]\n",
    "y_train = df_processed['Transported']\n",
    "\n",
    "print(f\"\\nğŸ“Š FILTERED TRAINING DATA SUMMARY:\")\n",
    "print(f\"   ğŸ“Š Original features: {df_processed.shape[1] - 2}\")  # Exclude PassengerId and Transported\n",
    "print(f\"   ğŸ“Š Selected features: {X_train.shape[1]}\")\n",
    "print(f\"   ğŸ“Š Feature reduction: {((df_processed.shape[1] - 2 - X_train.shape[1]) / (df_processed.shape[1] - 2) * 100):.1f}%\")\n",
    "print(f\"   ğŸ“Š Training samples: {X_train.shape[0]}\")\n",
    "print(f\"   ğŸ“Š Target distribution: {y_train.sum()}/{len(y_train)} transported\")\n",
    "\n",
    "print(f\"\\nğŸ¯ SELECTED FEATURES FOR OPTIMIZATION:\")\n",
    "for i, feature in enumerate(best_features, 1):\n",
    "    print(f\"   {i:2d}. {feature}\")\n",
    "\n",
    "print(f\"\\nâœ… Training data ready for hyperparameter optimization!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ LOADING TRAINING DATA WITH SELECTED FEATURES\n",
      "=======================================================\n",
      "   âœ… Training dataset loaded: (8693, 19)\n",
      "   âœ… Selected features loaded: 6 features\n",
      "\n",
      "ğŸ“Š FILTERED TRAINING DATA SUMMARY:\n",
      "   ğŸ“Š Original features: 17\n",
      "   ğŸ“Š Selected features: 6\n",
      "   ğŸ“Š Feature reduction: 64.7%\n",
      "   ğŸ“Š Training samples: 8693\n",
      "   ğŸ“Š Target distribution: 4378/8693 transported\n",
      "\n",
      "ğŸ¯ SELECTED FEATURES FOR OPTIMIZATION:\n",
      "    1. HomePlanet\n",
      "    2. CryoSleep\n",
      "    3. RoomService\n",
      "    4. TotalSpend\n",
      "    5. LuxurySpend\n",
      "    6. Cabin_HomePlanet\n",
      "\n",
      "âœ… Training data ready for hyperparameter optimization!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_processed.head(10)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2: Model creation, hyperparameter tuning, and validation"
   ],
   "metadata": {
    "id": "MQGrMwxMnHAX"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### STEP 1: Baseline Model Experimentation\n",
    "Evaluate both Tree-based and non-Tree ML models using K-fold CV to compare F1-macro, weighted F1, and accuracy scores. No feature engineering is applied. This serves as a baseline using raw numeric data for leaderboard benchmarking."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T19:03:42.914469Z",
     "start_time": "2025-06-06T19:03:32.951226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "# Define short descriptions\n",
    "model_descriptions = {\n",
    "    \"Decision Tree\": \"A simple, interpretable tree that splits data based on feature thresholds.\",\n",
    "    \"Random Forest\": \"An ensemble of decision trees that improves generalization via bagging.\",\n",
    "    \"Gradient Boosting\": \"A sequential ensemble where each tree corrects errors from the last.\",\n",
    "    \"AdaBoost\": \"A boosting method that emphasizes misclassified examples during training.\",\n",
    "    \"Logistic Regression\": \"A linear model that predicts probabilities for classification tasks.\",\n",
    "    \"SVM\": \"A margin-based classifier that finds the optimal boundary between classes using support vectors.\",\n",
    "    \"XGBoost\": \"A scalable, regularized boosting method with tree-based learners.\",\n",
    "    \"LightGBM\": \"A fast, efficient gradient boosting method based on histogram-based learning.\",\n",
    "    \"CatBoost\": \"A gradient boosting library with native support for categorical features.\"\n",
    "}\n",
    "\n",
    "# Define model types\n",
    "model_types = {\n",
    "    \"Decision Tree\": \"Tree-Based\",\n",
    "    \"Random Forest\": \"Tree-Based\",\n",
    "    \"Gradient Boosting\": \"Tree-Based\",\n",
    "    \"AdaBoost\": \"Tree-Based\",\n",
    "    \"XGBoost\": \"Tree-Based\",\n",
    "    \"CatBoost\": \"Tree-Based\",\n",
    "    \"Logistic Regression\": \"Non-Tree\",\n",
    "    \"SVM\": \"Non-Tree\"\n",
    "}\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=3, random_state=0),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=0),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=0),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=0),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=5000, random_state=0),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", C=1.0, probability=True, random_state=0),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=0),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0, random_state=0)\n",
    "}\n",
    "\n",
    "separator = \"-\" * 60\n",
    "results = {}\n",
    "\n",
    "print(\"\\nğŸ“Š Starting model evaluation with 5-fold cross-validation\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(separator)\n",
    "    print(f\"ğŸ” Model: {name}\")\n",
    "    print(f\"ğŸ§  Description: {model_descriptions.get(name, 'N/A')}\")\n",
    "    print(f\"âš™ï¸  Params: {model.get_params()}\")\n",
    "\n",
    "    cv_result = cross_validate(\n",
    "        model, X_train, y_train,\n",
    "        cv=5,\n",
    "        scoring=[\"f1_macro\", \"f1_weighted\", \"accuracy\"],\n",
    "        return_train_score=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Calculate means and standard deviations\n",
    "    results[name] = {\n",
    "        \"Model Type\": model_types[name],\n",
    "        \"Train F1 (Macro)\": np.mean(cv_result[\"train_f1_macro\"]),\n",
    "        \"Train F1 Std\": np.std(cv_result[\"train_f1_macro\"]),\n",
    "        \"CV F1 (Macro)\": np.mean(cv_result[\"test_f1_macro\"]),\n",
    "        \"CV F1 Std\": np.std(cv_result[\"test_f1_macro\"]),\n",
    "        \"CV F1 (Weighted)\": np.mean(cv_result[\"test_f1_weighted\"]),\n",
    "        \"CV F1 (W) Std\": np.std(cv_result[\"test_f1_weighted\"]),\n",
    "        \"CV Accuracy\": np.mean(cv_result[\"test_accuracy\"]),\n",
    "        \"CV Accuracy Std\": np.std(cv_result[\"test_accuracy\"])\n",
    "    }\n",
    "\n",
    "    # Print results with standard deviations\n",
    "    print(f\"âœ… Train F1 (Macro): {results[name]['Train F1 (Macro)']:.4f} (Â±{results[name]['Train F1 Std']:.4f})\")\n",
    "    print(f\"âœ… CV F1 (Macro): {results[name]['CV F1 (Macro)']:.4f} (Â±{results[name]['CV F1 Std']:.4f})\")\n",
    "    print(f\"âœ… CV F1 (Weighted): {results[name]['CV F1 (Weighted)']:.4f} (Â±{results[name]['CV F1 (W) Std']:.4f})\")\n",
    "    print(f\"âœ… CV Accuracy: {results[name]['CV Accuracy']:.4f} (Â±{results[name]['CV Accuracy Std']:.4f})\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\nâœ… All models evaluated. Summary below:\\n\")\n",
    "\n",
    "summary_df = pd.DataFrame(results).T.sort_values(\"CV Accuracy\", ascending=False)\n",
    "display_columns = [\"Model Type\", \"CV Accuracy\", \"CV Accuracy Std\", \"CV F1 (Macro)\", \"CV F1 (Weighted)\"]\n",
    "display(summary_df[display_columns].round(4))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Starting model evaluation with 5-fold cross-validation\n",
      "\n",
      "------------------------------------------------------------\n",
      "ğŸ” Model: Decision Tree\n",
      "ğŸ§  Description: A simple, interpretable tree that splits data based on feature thresholds.\n",
      "âš™ï¸  Params: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 0, 'splitter': 'best'}\n",
      "âœ… Train F1 (Macro): 0.7647 (Â±0.0039)\n",
      "âœ… CV F1 (Macro): 0.7600 (Â±0.0175)\n",
      "âœ… CV F1 (Weighted): 0.7602 (Â±0.0175)\n",
      "âœ… CV Accuracy: 0.7622 (Â±0.0166)\n",
      "------------------------------------------------------------\n",
      "ğŸ” Model: Random Forest\n",
      "ğŸ§  Description: An ensemble of decision trees that improves generalization via bagging.\n",
      "âš™ï¸  Params: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 0, 'verbose': 0, 'warm_start': False}\n",
      "âœ… Train F1 (Macro): 0.9134 (Â±0.0038)\n",
      "âœ… CV F1 (Macro): 0.7861 (Â±0.0096)\n",
      "âœ… CV F1 (Weighted): 0.7862 (Â±0.0095)\n",
      "âœ… CV Accuracy: 0.7867 (Â±0.0095)\n",
      "------------------------------------------------------------\n",
      "ğŸ” Model: Gradient Boosting\n",
      "ğŸ§  Description: A sequential ensemble where each tree corrects errors from the last.\n",
      "âš™ï¸  Params: {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 0, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "âœ… Train F1 (Macro): 0.8087 (Â±0.0021)\n",
      "âœ… CV F1 (Macro): 0.7944 (Â±0.0131)\n",
      "âœ… CV F1 (Weighted): 0.7945 (Â±0.0131)\n",
      "âœ… CV Accuracy: 0.7954 (Â±0.0127)\n",
      "------------------------------------------------------------\n",
      "ğŸ” Model: AdaBoost\n",
      "ğŸ§  Description: A boosting method that emphasizes misclassified examples during training.\n",
      "âš™ï¸  Params: {'algorithm': 'deprecated', 'estimator': None, 'learning_rate': 1.0, 'n_estimators': 100, 'random_state': 0}\n",
      "âœ… Train F1 (Macro): 0.7737 (Â±0.0038)\n",
      "âœ… CV F1 (Macro): 0.7713 (Â±0.0097)\n",
      "âœ… CV F1 (Weighted): 0.7713 (Â±0.0097)\n",
      "âœ… CV Accuracy: 0.7714 (Â±0.0098)\n",
      "------------------------------------------------------------\n",
      "ğŸ” Model: Logistic Regression\n",
      "ğŸ§  Description: A linear model that predicts probabilities for classification tasks.\n",
      "âš™ï¸  Params: {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 5000, 'multi_class': 'deprecated', 'n_jobs': None, 'penalty': 'l2', 'random_state': 0, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "âœ… Train F1 (Macro): 0.7839 (Â±0.0021)\n",
      "âœ… CV F1 (Macro): 0.7842 (Â±0.0066)\n",
      "âœ… CV F1 (Weighted): 0.7842 (Â±0.0066)\n",
      "âœ… CV Accuracy: 0.7845 (Â±0.0066)\n",
      "------------------------------------------------------------\n",
      "ğŸ” Model: SVM\n",
      "ğŸ§  Description: A margin-based classifier that finds the optimal boundary between classes using support vectors.\n",
      "âš™ï¸  Params: {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': True, 'random_state': 0, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "âœ… Train F1 (Macro): 0.7870 (Â±0.0029)\n",
      "âœ… CV F1 (Macro): 0.7858 (Â±0.0130)\n",
      "âœ… CV F1 (Weighted): 0.7859 (Â±0.0130)\n",
      "âœ… CV Accuracy: 0.7879 (Â±0.0122)\n",
      "------------------------------------------------------------\n",
      "ğŸ” Model: XGBoost\n",
      "ğŸ§  Description: A scalable, regularized boosting method with tree-based learners.\n",
      "âš™ï¸  Params: {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'logloss', 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 0, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n",
      "âœ… Train F1 (Macro): 0.8659 (Â±0.0027)\n",
      "âœ… CV F1 (Macro): 0.7912 (Â±0.0100)\n",
      "âœ… CV F1 (Weighted): 0.7912 (Â±0.0099)\n",
      "âœ… CV Accuracy: 0.7919 (Â±0.0098)\n",
      "------------------------------------------------------------\n",
      "ğŸ” Model: CatBoost\n",
      "ğŸ§  Description: A gradient boosting library with native support for categorical features.\n",
      "âš™ï¸  Params: {'verbose': 0, 'random_state': 0}\n",
      "âœ… Train F1 (Macro): 0.8344 (Â±0.0037)\n",
      "âœ… CV F1 (Macro): 0.8000 (Â±0.0077)\n",
      "âœ… CV F1 (Weighted): 0.8001 (Â±0.0077)\n",
      "âœ… CV Accuracy: 0.8008 (Â±0.0076)\n",
      "\n",
      "âœ… All models evaluated. Summary below:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                     Model Type CV Accuracy CV Accuracy Std CV F1 (Macro)  \\\n",
       "CatBoost             Tree-Based    0.800761        0.007552      0.800015   \n",
       "Gradient Boosting    Tree-Based    0.795356        0.012662      0.794396   \n",
       "XGBoost              Tree-Based    0.791904        0.009794       0.79118   \n",
       "SVM                    Non-Tree    0.787878        0.012199      0.785775   \n",
       "Random Forest        Tree-Based    0.786727        0.009458       0.78613   \n",
       "Logistic Regression    Non-Tree     0.78454        0.006646      0.784172   \n",
       "AdaBoost             Tree-Based    0.771428        0.009803      0.771305   \n",
       "Decision Tree        Tree-Based    0.762226        0.016611      0.760031   \n",
       "\n",
       "                    CV F1 (Weighted)  \n",
       "CatBoost                    0.800085  \n",
       "Gradient Boosting            0.79449  \n",
       "XGBoost                     0.791243  \n",
       "SVM                         0.785923  \n",
       "Random Forest               0.786183  \n",
       "Logistic Regression         0.784225  \n",
       "AdaBoost                    0.771296  \n",
       "Decision Tree               0.760193  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Type</th>\n",
       "      <th>CV Accuracy</th>\n",
       "      <th>CV Accuracy Std</th>\n",
       "      <th>CV F1 (Macro)</th>\n",
       "      <th>CV F1 (Weighted)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>Tree-Based</td>\n",
       "      <td>0.800761</td>\n",
       "      <td>0.007552</td>\n",
       "      <td>0.800015</td>\n",
       "      <td>0.800085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>Tree-Based</td>\n",
       "      <td>0.795356</td>\n",
       "      <td>0.012662</td>\n",
       "      <td>0.794396</td>\n",
       "      <td>0.79449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>Tree-Based</td>\n",
       "      <td>0.791904</td>\n",
       "      <td>0.009794</td>\n",
       "      <td>0.79118</td>\n",
       "      <td>0.791243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>Non-Tree</td>\n",
       "      <td>0.787878</td>\n",
       "      <td>0.012199</td>\n",
       "      <td>0.785775</td>\n",
       "      <td>0.785923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>Tree-Based</td>\n",
       "      <td>0.786727</td>\n",
       "      <td>0.009458</td>\n",
       "      <td>0.78613</td>\n",
       "      <td>0.786183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>Non-Tree</td>\n",
       "      <td>0.78454</td>\n",
       "      <td>0.006646</td>\n",
       "      <td>0.784172</td>\n",
       "      <td>0.784225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>Tree-Based</td>\n",
       "      <td>0.771428</td>\n",
       "      <td>0.009803</td>\n",
       "      <td>0.771305</td>\n",
       "      <td>0.771296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>Tree-Based</td>\n",
       "      <td>0.762226</td>\n",
       "      <td>0.016611</td>\n",
       "      <td>0.760031</td>\n",
       "      <td>0.760193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 74.6 ms, sys: 100 ms, total: 175 ms\n",
      "Wall time: 9.95 s\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary of Baseline Model Results\n",
    "\n",
    "After engineering 17 high-impact features from the raw spaceship data, we evaluated eight machine learning algorithms using a structured performance framework. From basic decision trees to advanced ensemble models like CatBoost, each model was tested through 5-fold cross-validation.\n",
    "\n",
    "**Key Findings:**\n",
    "- **CatBoost emerged as the clear winner**, achieving 80.08% accuracy with exceptional stability (0.75% standard deviation)\n",
    "- **Tree-based models dominated the leaderboard**, occupying 6 of the top 8 positions\n",
    "- **Gradient Boosting and XGBoost followed closely** at 79.53% and 79.19% respectively\n",
    "- **Non-tree models showed competitive performance**, with SVM reaching 78.79% accuracy\n",
    "\n",
    "**Performance Insights:**\n",
    "- **Top 3 models all exceeded 79% accuracy**, indicating strong feature engineering effectiveness\n",
    "- **CatBoost's low variance** (0.75% std) suggests robust performance across different data splits\n",
    "- **Significant performance gap** between top performers (80%+) and basic models like Decision Tree (76%)\n",
    "\n",
    "**Strategic Implications:**\n",
    "The baseline results validate our feature engineering approach and identify **CatBoost, Gradient Boosting, and XGBoost** as prime candidates for hyperparameter optimization. With the current baseline exceeding 80% accuracy, the foundation is set for advanced model tuning to push performance even higher."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T19:19:56.061487Z",
     "start_time": "2025-06-06T19:19:53.881760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Models to test\n",
    "submission_models = {\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0, random_state=0),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=100, random_state=0),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=0)\n",
    "}\n",
    "\n",
    "# Prepare test data\n",
    "X_test_kaggle = pd.read_csv('../data/processed/test_features_engineered.csv')\n",
    "passenger_ids = X_test_kaggle[\"PassengerId\"]\n",
    "X_test_kaggle = X_test_kaggle[best_features]\n",
    "# X_test_kaggle = X_test_kaggle.drop(\"PassengerId\", axis=1)\n",
    "\n",
    "# Generate and save predictions for each model\n",
    "for name, model in submission_models.items():\n",
    "    print(f\"\\nğŸš€ Training and predicting with: {name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test_kaggle)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        \"PassengerId\": passenger_ids,\n",
    "        \"Transported\": preds.astype(bool)\n",
    "    })\n",
    "\n",
    "    submission.to_csv(f\"submissions/submission_best_feats_{name.lower()}.csv\", index=False)\n",
    "    print(f\"âœ… Submission file created: submission_best_feats_{name.lower()}.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Training and predicting with: CatBoost\n",
      "âœ… Submission file created: submission_best_feats_catboost.csv\n",
      "\n",
      "ğŸš€ Training and predicting with: GradientBoosting\n",
      "âœ… Submission file created: submission_best_feats_gradientboosting.csv\n",
      "\n",
      "ğŸš€ Training and predicting with: XGBoost\n",
      "âœ… Submission file created: submission_best_feats_xgboost.csv\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
